---
phase: 04-gymnasium-environment-wrapper
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/ai/ai-config.ts
  - src/ai/raycaster.ts
  - src/ai/observations.ts
  - src/ai/reward.ts
  - tests/ai/raycaster.test.ts
  - tests/ai/observations.test.ts
  - tests/ai/reward.test.ts
autonomous: true
requirements: [AI-02, AI-03, AI-04, AI-05, AI-06, AI-12, AI-13]

must_haves:
  truths:
    - "9 rays cast across 180-degree forward arc return normalized distances to boundary walls"
    - "14-value observation vector contains all required components normalized to [-1,1] or [0,1]"
    - "Dense per-tick reward uses continuous arc-length progress (primary), not discrete checkpoint steps"
    - "Four-tier penalties (stillness, wall, off-track, backward) are computed separately"
    - "Penalties are always smaller in magnitude than typical progress rewards"
    - "Reward weights are loaded from a config object (no hardcoding)"
    - "Each reward component is returned individually for per-component logging"
  artifacts:
    - path: "src/ai/ai-config.ts"
      provides: "RewardConfig type, default weights, episode config, observation/ray constants"
      exports: ["RewardConfig", "EpisodeConfig", "AiConfig", "DEFAULT_AI_CONFIG", "RAY", "OBS"]
    - path: "src/ai/raycaster.ts"
      provides: "9-ray cast against track boundary polylines"
      exports: ["castRays", "raySegmentIntersection"]
    - path: "src/ai/observations.ts"
      provides: "14-value normalized observation vector builder"
      exports: ["buildObservation", "OBSERVATION_SIZE"]
    - path: "src/ai/reward.ts"
      provides: "Reward computation with per-component breakdown"
      exports: ["computeReward", "RewardBreakdown"]
    - path: "tests/ai/raycaster.test.ts"
      provides: "Ray casting unit tests"
    - path: "tests/ai/observations.test.ts"
      provides: "Observation vector unit tests"
    - path: "tests/ai/reward.test.ts"
      provides: "Reward computation unit tests"
  key_links:
    - from: "src/ai/raycaster.ts"
      to: "src/engine/types.ts"
      via: "imports Vec2 for ray casting against innerBoundary/outerBoundary"
      pattern: "import.*Vec2.*from.*engine/types"
    - from: "src/ai/observations.ts"
      to: "src/engine/constants.ts"
      via: "imports CAR.maxSpeed for speed normalization"
      pattern: "import.*CAR.*from.*engine/constants"
    - from: "src/ai/reward.ts"
      to: "src/ai/ai-config.ts"
      via: "reads RewardConfig weights for component computation"
      pattern: "import.*RewardConfig.*from.*ai-config"
    - from: "src/ai/reward.ts"
      to: "src/engine/track.ts"
      via: "imports distanceToTrackCenter for continuous arc-length progress"
      pattern: "import.*distanceToTrackCenter.*from.*engine/track"
---

## Enhancement Summary

**Deepened on:** 2026-03-01
**Research agents used:** 11 (TypeScript reviewer, performance oracle, architecture strategist, pattern recognition specialist, code simplicity reviewer, security sentinel, RL best practices researcher, Gymnasium docs researcher, spec flow analyzer, engine interface verifier, Context7 docs)
**External sources:** Nature 2025 paper on reward design, F1Tenth RL studies, PMC LiDAR configuration study, Gymnasium/SB3 official docs

### Critical Issues Found and Resolved
1. **Progress reward was sparse, not dense** -- plan used discrete checkpoint arc-length; fixed to use continuous `distanceToTrackCenter().arcLength`
2. **Wall detection after collision resolution returns false** -- `stepWorld` resolves collision before reward sees it; fixed by accepting `wallContact: boolean` parameter
3. **`computeReward` signature instability** -- resolved: takes `RewardConfig` + `wallContact`, not `AiConfig`
4. **Reward weight imbalance** -- speed bonus at 0.1 would dominate progress by 37x; defaults rebalanced

### Key Improvements
1. Move `stillnessSpeedThreshold` into `RewardConfig` (reward concern, not episode concern)
2. Extract all magic numbers to named constants in `ai-config.ts`
3. `buildObservation` accepts pre-computed `centerlineDist` and `arcLength` to avoid redundant `distanceToTrackCenter` calls
4. File naming aligned to codebase conventions (camelCase for pure-function modules)
5. `as const` on DEFAULT_AI_CONFIG to match engine pattern
6. Default reward weights rebalanced per RL literature: penalties < typical progress per tick

### New Considerations Discovered
- Consider increasing rays to 19+ and FOV to 220deg in Phase 5 tuning (research shows 9 rays insufficient for complex tracks)
- Consider adding heading error observation (angle to track tangent) in Phase 5
- Consider signed centerline distance [-1,1] instead of unsigned [0,1] for directional info
- Pre-compute ray angle offsets as module-level constants to avoid per-tick allocation
- Segment-window pruning for `castRays` deferred to Phase 5 profiling

---

<objective>
Build the three core AI computation modules: ray casting, observation vector, and reward function. These are pure TypeScript functions that run engine-side with well-defined I/O -- ideal for TDD.

Purpose: These modules provide the AI agent's sensory input (what it sees) and feedback signal (what it learns from). They must be correct, normalized, and testable independently from the bridge layer.

Output: Four TypeScript source files in `src/ai/` with comprehensive test coverage in `tests/ai/`. All computation happens engine-side (zero Python, zero network).
</objective>

<execution_context>
@~/.claude/commands/gsd/workflows/execute-plan.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md

<interfaces>
<!-- Key types and contracts the executor needs from the engine. -->
<!-- VERIFIED: All signatures below confirmed against actual source code 2026-03-01 -->

From src/engine/types.ts:
```typescript
export interface Vec2 { readonly x: number; readonly y: number; }
export const enum Surface { Road = 0, Runoff = 1, Shoulder = 2 }
export interface Input { steer: number; throttle: number; brake: number; }
export interface SmoothedInput { steer: number; throttle: number; brake: number; steerAngle: number; }
export interface CarState {
  position: Vec2; velocity: Vec2; heading: number; yawRate: number;
  speed: number; prevInput: SmoothedInput; surface: Surface;
  accelLongitudinal: number; slipAngle: number;
}
export interface Checkpoint { left: Vec2; right: Vec2; center: Vec2; direction: Vec2; arcLength: number; }
export interface TrackState {
  controlPoints: readonly TrackControlPoint[];
  innerBoundary: readonly Vec2[]; outerBoundary: readonly Vec2[];
  innerRoadEdge: readonly Vec2[]; outerRoadEdge: readonly Vec2[];
  checkpoints: readonly Checkpoint[];
  arcLengthTable: ArcLengthTable;
  totalLength: number; startPosition: Vec2; startHeading: number;
}
export interface TimingState {
  currentLapTicks: number; bestLapTicks: number; totalRaceTicks: number;
  currentLap: number; lastCheckpointIndex: number; lapComplete: boolean;
  lapTimes: readonly number[];
}
export interface WorldState { tick: number; car: CarState; track: TrackState; timing: TimingState; }
export interface CollisionResult { collided: boolean; penetration: number; normal: Vec2; contactPoint: Vec2; }
```

From src/engine/constants.ts:
```typescript
export const CAR = { mass: 800, maxSpeed: 160, width: 2.0, /* ... */ } as const;
```

From src/engine/vec2.ts:
```typescript
export function vec2(x: number, y: number): Vec2;
export function sub(a: Vec2, b: Vec2): Vec2;
export function dot(a: Vec2, b: Vec2): number;
export function length(v: Vec2): number;
export function normalize(v: Vec2): Vec2;
export function fromAngle(angle: number): Vec2;
// ... plus add, scale, cross, rotate, distance, etc.
```

From src/engine/track.ts:
```typescript
export function distanceToTrackCenter(position: Vec2, track: TrackState): { distance: number; arcLength: number };
export function getSurface(position: Vec2, track: TrackState): Surface;
export const WALL_OFFSET = 30;
```

From src/engine/collision.ts:
```typescript
export function detectWallCollision(position: Vec2, radius: number, track: TrackState): CollisionResult;
export function pointToSegmentDistance(point: Vec2, segA: Vec2, segB: Vec2): { distance: number; nearest: Vec2; t: number };
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AI config types and ray caster with TDD</name>
  <files>
    src/ai/ai-config.ts
    src/ai/raycaster.ts
    tests/ai/raycaster.test.ts
  </files>
  <action>
**Step 1: Create `src/ai/ai-config.ts`** -- Type definitions, constants, and default config.

### Research Insights

**Config Design (Simplicity Review + Architecture Review):**
- Move `stillnessSpeedThreshold` from `EpisodeConfig` into `RewardConfig` -- it is a reward concern (defines when the stillness penalty fires). This lets `computeReward` take `RewardConfig` cleanly without needing full `AiConfig`.
- Use `as const` on `DEFAULT_AI_CONFIG` to match the engine's pattern (`CAR`, `TIRE`, etc. all use `as const`).
- Extract ALL magic numbers into named constants rather than hardcoding in function bodies.

**Reward Weight Defaults (RL Research -- Nature 2025, F1Tenth, PMC):**
- With typical track length ~1000 units, maxSpeed=160, DT=1/60: progress per tick at moderate speed is ~0.002-0.004 (normalized by totalLength).
- Speed bonus at 0.1 would dominate progress by ~37x -- agent floors throttle into walls. Start at 0.0 and enable during Phase 5 tuning.
- Wall penalty at -0.05 erases ~17 ticks of progress per contact tick -- teaches stillness. Use -0.002.
- Backward penalty at -0.1 is redundant with negative progress delta. Start at 0.0.
- All penalties must be smaller than typical per-tick progress (~0.003). Set penalties at -0.001 to -0.002.

**Naming Convention (Pattern Review):**
- File should be `ai-config.ts` (single-word or kebab-case matches existing: `vec2.ts`, `types.ts`). Note: the existing codebase has `formatTime.ts` (camelCase multi-word) but no kebab-case. Use `aiConfig.ts` to match `formatTime.ts` convention if preferred, but `ai-config.ts` is acceptable since the plan already names it and it is consistent within `src/ai/`.

```typescript
/** Ray casting configuration constants. */
export const RAY = {
  numRays: 9,
  fovRadians: Math.PI,    // 180 degrees
  maxDist: 200,            // game units
} as const;

/** Observation normalization constants. */
export const OBS = {
  /** Max expected yaw rate (rad/s) for normalization. Derived from max cornering dynamics. */
  maxYawRate: 5.0,
  /** Max expected centerline distance (game units). Conservative upper bound for road width + WALL_OFFSET. */
  maxCenterlineDist: 80,
  /** Number of observation values in the vector. */
  size: 14,
} as const;

export interface RewardConfig {
  progress: number;              // Weight for continuous arc-length progress (primary signal)
  speedBonus: number;            // Weight for normalized speed bonus
  wallPenalty: number;           // Negative weight for wall collision (per tick of contact)
  offTrackPenalty: number;       // Negative weight for off-track (runoff/shoulder)
  backwardPenalty: number;       // Negative weight for backward driving
  stillnessPenalty: number;      // Negative weight for being too slow
  stillnessSpeedThreshold: number; // Speed below this counts as "still" (moved from EpisodeConfig)
}

export interface EpisodeConfig {
  maxSteps: number;              // Max ticks per episode (3000 = 50s at 60Hz)
  stillnessTimeoutTicks: number; // Ticks below speed threshold before termination
}

export interface AiConfig {
  weights: RewardConfig;
  episode: EpisodeConfig;
}

export const DEFAULT_AI_CONFIG = {
  weights: {
    progress: 1.0,
    speedBonus: 0.0,              // Disabled by default; progress already rewards speed implicitly
    wallPenalty: -0.002,          // < typical progress per tick (~0.003)
    offTrackPenalty: -0.001,      // Gentle nudge back to road
    backwardPenalty: 0.0,         // Redundant: negative progress delta handles this
    stillnessPenalty: -0.001,     // Gentle nudge to move; termination handles extended stillness
    stillnessSpeedThreshold: 2.0,
  },
  episode: {
    maxSteps: 3000,
    stillnessTimeoutTicks: 180,
  },
} as const satisfies AiConfig;
```

### Performance Considerations
- Constants are module-level `as const` objects -- zero allocation per tick.
- Pre-computed ray angle offsets could be added here or in raycaster.ts.

---

**Step 2: RED -- Write failing tests for `raycaster.ts`** in `tests/ai/raycaster.test.ts`.

Test cases for `raySegmentIntersection(origin, direction, segA, segB)`:
- Ray hits a horizontal segment perpendicular -> returns correct distance
- Ray hits a vertical segment -> returns correct distance
- Ray parallel to segment -> returns null
- Ray pointing away from segment -> returns null
- Ray misses segment (passes to the side) -> returns null
- Segment behind ray origin -> returns null

Test cases for `castRays(carPosition, carHeading, innerBoundary, outerBoundary)`:
- Create a simple rectangular "track" with 4 boundary segments forming a box (inner + outer boundaries as Vec2[])
- Car at center, heading right (0 radians): verify 9 rays return distances to the box walls
- All ray values are normalized to [0, 1] (divided by maxDist)
- Car near a wall: the closest ray should have a small value, far rays should be larger
- No wall within maxDist: ray returns 1.0
- **NEW: Nearest-hit test** -- ray intersects both inner and outer boundary; verify minimum distance is returned
- **NEW: All rays within [0, 1] bounds** -- assert every value satisfies `0 <= v <= 1`

### Research Insights -- Additional Test Cases
- **Edge case: car exactly on a boundary point** -- should return 0.0 for the ray pointing along that segment
- **Edge case: car outside boundaries** -- all rays may miss; should return 1.0 (maxDist)

Use `buildTrack` from `src/engine/track.ts` with a simple track to create realistic test boundaries, OR construct synthetic boundary polylines directly for unit-level isolation. Prefer synthetic for unit tests.

**Step 3: GREEN -- Implement `src/ai/raycaster.ts`.**

```typescript
/** Compute intersection distance of a ray with a line segment. Returns distance (t) or null if no hit. */
export function raySegmentIntersection(
  origin: Vec2, direction: Vec2, segA: Vec2, segB: Vec2
): number | null
```
Uses the parametric line-segment intersection formula. Same math as `checkGateCrossing` in checkpoint.ts. Ray: `P = origin + t * direction` (t >= 0). Segment: `Q = segA + u * (segB - segA)` (u in [0, 1]). Return t (distance) if hit, null if miss.

```typescript
/** Cast rays from car position across forward arc, returning normalized distances to nearest boundary hit. */
export function castRays(
  carPosition: Vec2, carHeading: number,
  innerBoundary: readonly Vec2[], outerBoundary: readonly Vec2[],
): number[]
```

### Research Insights

**API Design (Simplicity Review):**
- Remove optional `numRays`, `fovRadians`, `maxDist` parameters. The requirements specify exactly 9 rays, 180deg, and these are fixed for the entire project lifecycle. Read from the `RAY` constant object instead. If values ever need changing, update one constant. Three fewer parameters to document and test.

**Performance (Performance Oracle):**
- Pre-compute relative ray angle offsets as a module-level constant to avoid per-tick allocation:
  ```typescript
  const RAY_OFFSETS = Array.from({ length: RAY.numRays }, (_, i) =>
    -RAY.fovRadians / 2 + (i * RAY.fovRadians) / (RAY.numRays - 1)
  );
  ```
- Use raw `Math.cos(angle)` / `Math.sin(angle)` for direction instead of creating Vec2 objects per ray.
- At 3000+ ticks/sec with ~200 boundary segments per boundary, brute-force iteration is ~3,600 ray-segment tests per tick. This is acceptable for Phase 4. If profiling in Phase 5 shows castRays as a bottleneck, add segment-window pruning (test only segments within maxDist of car's arc-length position) for a 50-75% reduction.

**Implementation:**
Cast rays from car position at angles `heading + RAY_OFFSETS[i]`. For each ray, test against ALL segments of both `innerBoundary` and `outerBoundary`. Return `min(hitDist, RAY.maxDist) / RAY.maxDist` -- normalized to [0, 1].

**Important:** The function takes `innerBoundary` and `outerBoundary` as separate arrays (not the full TrackState) to keep the API clean and testable. The caller (headless-env) extracts boundaries from TrackState.

Run `pnpm test -- tests/ai/raycaster.test.ts` -- all tests must pass.
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02 && pnpm test -- tests/ai/raycaster.test.ts</automated>
  </verify>
  <done>
    - ai-config.ts exports RewardConfig, EpisodeConfig, AiConfig types, DEFAULT_AI_CONFIG constant, RAY and OBS constant objects
    - raySegmentIntersection returns distance on hit, null on miss
    - castRays returns 9 normalized [0,1] values for a car position/heading against boundaries
    - Ray angle offsets pre-computed as module-level constant
    - All raycaster tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Create observation vector builder and reward function with TDD</name>
  <files>
    src/ai/observations.ts
    src/ai/reward.ts
    tests/ai/observations.test.ts
    tests/ai/reward.test.ts
  </files>
  <action>
**Step 1: RED -- Write failing tests for `observations.ts`** in `tests/ai/observations.test.ts`.

Test cases for `buildObservation(world, rays, trackProgress)`:
- Returns array of exactly 14 numbers
- First 9 values are the ray values passed in (unchanged)
- Value 10: speed normalized to [0, 1] via `car.speed / CAR.maxSpeed`
- Value 11: yaw rate normalized to [-1, 1] via `clamp(car.yawRate / OBS.maxYawRate, -1, 1)`
- Value 12: steering input [-1, 1] from `car.prevInput.steer` (already in range)
- Value 13: lap progress [0, 1] from `arcLength / track.totalLength` (continuous, NOT discrete checkpoint)
- Value 14: centerline distance normalized to [0, 1] via `min(1, distance / OBS.maxCenterlineDist)`
- All values are within [-1, 1] range (the union of all component ranges)
- Edge cases: speed at max, yaw rate beyond clamp range, car at start position

### Research Insights

**Signature Change (Performance Oracle + Architecture Review):**
- `buildObservation` should accept pre-computed `trackProgress: { distance: number; arcLength: number }` as a third parameter instead of calling `distanceToTrackCenter` internally. This avoids a redundant O(500+) spline evaluation per tick -- the caller (HeadlessEnv in Plan 02) already needs this value for `computeReward` and can compute it once.

**Type Safety (TypeScript Review):**
- Export `OBSERVATION_SIZE = 14 as const` for downstream consumers (Python bridge needs to know the vector size).
- Add a runtime assertion `if (rays.length !== RAY.numRays)` in development builds to catch integration bugs early.

**Normalization (RL Research):**
- Use named constants from `OBS` config object instead of magic numbers 5.0 and 50.
- The centerline distance normalization denominator was 50 in the original plan but the actual max distance (road half-width + WALL_OFFSET) can reach 60+. Updated to 80 via `OBS.maxCenterlineDist`.
- Consider signed centerline distance (left/right) in Phase 5 for more informative observation. For Phase 4, unsigned [0,1] is sufficient and matches the requirement.

**Observation Vector Future Expansion (RL Research):**
- 9 rays with 180deg FOV is the minimum viable configuration. PMC study found 10 beams had only 12% success in complex environments; 20 beams achieved 98%.
- Consider adding heading error (angle between car heading and track tangent) as a 15th observation in Phase 5.
- Consider expanding to 19 rays with 220deg FOV in Phase 5 if the agent struggles with side-wall awareness during oversteer.
- These changes are deferred because they affect the observation_space shape in the Gymnasium wrapper, which should be stable for initial training runs.

Use mock WorldState objects (no need to run the engine). Create minimal mock with required fields.

**Step 2: GREEN -- Implement `src/ai/observations.ts`.**

```typescript
export const OBSERVATION_SIZE = 14;

/** Build the 14-value normalized observation vector from world state, rays, and pre-computed track info. */
export function buildObservation(
  world: WorldState,
  rays: number[],
  trackProgress: { distance: number; arcLength: number },
): number[]
```

Builds the 14-value vector per AI-03:
1. rays[0..8] -- 9 ray distances, already normalized to [0, 1]
2. `car.speed / CAR.maxSpeed` -- normalized speed [0, 1]
3. `clamp(car.yawRate / OBS.maxYawRate, -1, 1)` -- angular velocity [-1, 1]
4. `car.prevInput.steer` -- current steering [-1, 1]
5. `trackProgress.arcLength / track.totalLength` -- lap progress [0, 1] (continuous, smooth at lap boundary)
6. `min(1, trackProgress.distance / OBS.maxCenterlineDist)` -- centerline distance [0, 1]

Import `CAR` from `../engine/constants` for maxSpeed. Import `OBS` from `./ai-config` for normalization constants.

**CRITICAL: Use continuous arc-length from `trackProgress.arcLength`, NOT discrete checkpoint-based progress.** Using `timing.lastCheckpointIndex` would produce a sparse staircase signal that only changes every ~60-120 ticks at checkpoint crossings. The RL agent needs dense per-tick signal. The caller (HeadlessEnv) computes `trackProgress` via `distanceToTrackCenter(car.position, track)`.

---

**Step 3: RED -- Write failing tests for `reward.ts`** in `tests/ai/reward.test.ts`.

Test cases for `computeReward(prevWorld, currWorld, wallContact, config)`:
- Forward progress between ticks produces positive reward (arc-length delta > 0)
- **CRITICAL: Progress reward is dense** -- even small forward movement produces nonzero progress (verified with two close positions, not two different checkpoint indices)
- Speed bonus is proportional to `car.speed / CAR.maxSpeed * config.speedBonus`
- Wall contact (wallContact=true) applies `config.wallPenalty`
- Off-track surface (Surface.Runoff or Surface.Shoulder) applies `config.offTrackPenalty`
- Backward driving (negative progress delta) applies `config.backwardPenalty`
- Stillness (speed below `config.stillnessSpeedThreshold`) applies `config.stillnessPenalty`
- Total reward is sum of all components
- Each component is returned individually in `RewardBreakdown` (AI-13)
- With default config, penalty magnitudes are smaller than typical progress reward (AI-06)
- Progress wrapping at lap boundary works correctly (modular arithmetic)
- Configuring different weights produces different reward values (AI-12)
- **NEW: First tick after reset** -- car at rest, speed=0, minimal progress. Stillness penalty fires but is small.
- **NEW: Multiple simultaneous penalties** -- wall + off-track + stillness all active at once; total equals sum of all components.
- **NEW: Progress at lap wrap** -- car crosses start/finish line; delta should be a small positive value, not a large negative.

### Research Insights

**Dense Reward -- CRITICAL FIX (TypeScript Review + Architecture Review + Spec Flow):**

The original plan used `track.checkpoints[timing.lastCheckpointIndex].arcLength` for progress calculation. This is a **sparse signal** -- the arc-length only changes when the car crosses a checkpoint gate (~30 per lap, so roughly every 1-2 seconds). Between checkpoints, `prevArc === currArc` and progress reward is zero. This violates AI-04 ("dense per-tick reward").

**FIX:** Use `distanceToTrackCenter(car.position, track).arcLength` for both `prevWorld` and `currWorld` to get the car's continuous position along the centerline. This gives a small positive delta every tick the car moves forward.

```typescript
// WRONG (sparse -- only changes at checkpoint crossings):
const prevArc = track.checkpoints[prevWorld.timing.lastCheckpointIndex].arcLength;
const currArc = track.checkpoints[currWorld.timing.lastCheckpointIndex].arcLength;

// CORRECT (dense -- changes every tick as car moves):
const prevArc = distanceToTrackCenter(prevWorld.car.position, prevWorld.track).arcLength;
const currArc = distanceToTrackCenter(currWorld.car.position, currWorld.track).arcLength;
```

**Wall Detection -- CRITICAL FIX (Architecture Review + Performance Oracle):**

The original plan proposed calling `detectWallCollision` inside `reward.ts`. This has two problems:
1. **Correctness:** After `stepWorld` resolves collision, the car is pushed away from the wall. `detectWallCollision` on the resolved position returns `collided: false` because the resolution moved the car exactly to `radius` distance from the wall -- the threshold where `collided` goes false.
2. **Performance:** Redundant O(n) boundary scan. `stepWorld` already computed this.

**FIX:** Accept `wallContact: boolean` as a parameter. HeadlessEnv (Plan 02) will detect wall contact BEFORE collision resolution and pass the boolean in:

```typescript
export function computeReward(
  prevWorld: WorldState,
  currWorld: WorldState,
  wallContact: boolean,
  config: RewardConfig,
): RewardBreakdown
```

This keeps `reward.ts` as a pure calculator with no engine imports beyond types.

**Reward Weight Balance (Nature 2025 + F1Tenth + PMC):**
- Progress per tick at moderate speed (~80 units/s, 60Hz, 1000-unit track): `(80/60) / 1000 = 0.00133` raw, * weight 1.0 = 0.00133.
- Wall penalty at -0.002 < progress: Agent can always earn more by driving forward than it loses from one tick of wall contact. This satisfies AI-06.
- Speed bonus at 0.0 by default: Progress already implicitly rewards speed (faster car = more arc-length per tick). Explicit bonus risks teaching "floor the throttle into walls." Enable during Phase 5 tuning only if agent drives safely but too slowly.
- Backward penalty at 0.0: Negative progress delta already makes the progress component negative. Adding a separate penalty double-punishes.

**Naming (Pattern Review):**
- Use `offTrackPenalty` consistently (capital T) in both `RewardConfig` and `RewardBreakdown`, matching codebase camelCase convention (`accelLongitudinal`, `yawRate`).

```typescript
export interface RewardBreakdown {
  progress: number;
  speed: number;
  wall: number;
  offTrack: number;
  backward: number;
  stillness: number;
  total: number;
}
```

**Step 4: GREEN -- Implement `src/ai/reward.ts`.**

```typescript
/** Compute per-tick reward with component breakdown. Pure function -- no engine queries. */
export function computeReward(
  prevWorld: WorldState,
  currWorld: WorldState,
  wallContact: boolean,
  config: RewardConfig,
): RewardBreakdown
```

**Progress calculation:** Import `distanceToTrackCenter` from `../engine/track`. Get continuous arc-length for prevWorld and currWorld car positions. Compute delta with modular wrap:

```typescript
const prevArc = distanceToTrackCenter(prevWorld.car.position, prevWorld.track).arcLength;
const currArc = distanceToTrackCenter(currWorld.car.position, currWorld.track).arcLength;
const totalLen = currWorld.track.totalLength;
let delta = currArc - prevArc;
// Handle wrap-around at lap boundary
if (delta < -totalLen / 2) delta += totalLen;
if (delta > totalLen / 2) delta -= totalLen;
const progressReward = (delta / totalLen) * config.progress;
```

### Performance Considerations

**Redundant `distanceToTrackCenter` calls (Performance Oracle):**
- `distanceToTrackCenter` performs ~500+ spline evaluations per call.
- In the naive implementation, it would be called 3x per tick: once in `buildObservation` (centerline dist), once for `prevWorld` progress, once for `currWorld` progress.
- **Optimization for Plan 02:** HeadlessEnv should compute `distanceToTrackCenter` once per world state and pass the result to both `buildObservation` and `computeReward`. For Plan 01, `reward.ts` calls it internally (correct but has redundancy that Plan 02 resolves).
- To enable this optimization, `computeReward` should accept an optional `prevArcLength` and `currArcLength` parameter. If provided, skip the internal `distanceToTrackCenter` calls. If not provided (for standalone testing), compute internally:

```typescript
export function computeReward(
  prevWorld: WorldState,
  currWorld: WorldState,
  wallContact: boolean,
  config: RewardConfig,
  precomputed?: { prevArcLength: number; currArcLength: number },
): RewardBreakdown
```

**Speed bonus:** `(currWorld.car.speed / CAR.maxSpeed) * config.speedBonus`. With default `speedBonus: 0.0`, this component is zero. No computation wasted.

**Wall penalty:** `wallContact ? config.wallPenalty : 0`. Simple boolean check, zero engine queries.

**Off-track:** `currWorld.car.surface !== Surface.Road ? config.offTrackPenalty : 0`.

**Backward:** `delta < 0 ? config.backwardPenalty : 0`. With default `backwardPenalty: 0.0`, this is unused. The negative progress delta already penalizes backward movement.

**Stillness:** `currWorld.car.speed < config.stillnessSpeedThreshold ? config.stillnessPenalty : 0`.

**Total:** Sum all components. Return `RewardBreakdown` with each value individually.

Run `pnpm test -- tests/ai/observations.test.ts tests/ai/reward.test.ts` -- all tests must pass.

Also run the full test suite to verify no regressions: `pnpm test`.

### Edge Cases

- **First tick after reset:** Car at rest (speed=0), minimal arc-length movement. Stillness penalty fires (-0.001) with near-zero progress. Net reward slightly negative. This is expected and acceptable -- the agent learns to start moving.
- **Lap boundary wrap:** `delta` computation uses `if (delta < -totalLen/2) delta += totalLen` to handle the wrap from `totalLength` back to 0. If the car crosses start/finish moving forward at 80 units/s, `delta` is ~1.33 units, producing a small positive reward regardless of the wrap.
- **Multiple simultaneous penalties:** Car going backward on shoulder while slow: offTrack + backward + stillness all fire. Total is simply the sum. No early returns or short-circuiting.
- **All rays miss (car outside boundaries):** All rays return 1.0 (maxDist). Observation is valid. Reward will likely include offTrack penalty from surface check.

### References

- [Reward design for autonomous racing (Nature, 2025)](https://www.nature.com/articles/s41598-025-27702-6) -- Progress-based rewards + penalty structures; PPO hyperparameters
- [F1Tenth Autonomous Racing with Offline RL (2024)](https://arxiv.org/html/2408.04198v1) -- 20-beam LiDAR, progress-based reward
- [Impact of LiDAR Configuration (PMC, 2023)](https://pmc.ncbi.nlm.nih.gov/articles/PMC10747335/) -- 10 beams: 12% success; 20 beams: 98% success
- [Gymnasium custom env docs](https://gymnasium.farama.org/introduction/create_custom_env/) -- Box spaces, check_env, terminated/truncated
- [SB3 custom env guide](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html) -- float32 requirement, VecNormalize patterns
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02 && pnpm test -- tests/ai/observations.test.ts tests/ai/reward.test.ts</automated>
  </verify>
  <done>
    - buildObservation returns exactly 14 normalized values: 9 rays + speed + yawRate + steering + lapProgress + centerlineDist
    - buildObservation uses pre-computed trackProgress (no internal distanceToTrackCenter call)
    - computeReward returns RewardBreakdown with all 6 components + total
    - computeReward uses continuous arc-length (dense signal, not sparse checkpoint-based)
    - computeReward accepts wallContact boolean (no internal collision detection)
    - Progress wrapping at lap boundary produces correct positive delta
    - Default config satisfies AI-06: penalties smaller than progress rewards
    - Configurable weights change reward output (AI-12)
    - First-tick and multi-penalty edge cases tested
    - All observation and reward tests pass
    - Full test suite passes (no regressions)
  </done>
</task>

</tasks>

<verification>
1. `pnpm test` -- all tests pass including new ai/ tests and existing engine/ tests
2. `pnpm exec tsc --noEmit` -- no TypeScript errors
3. All 4 new source files exist in src/ai/ with correct exports
4. All 3 new test files exist in tests/ai/ with passing assertions
5. Ray values are always in [0, 1], observation values in [-1, 1]
6. Reward breakdown total equals sum of components
7. Progress reward is dense: nonzero on ticks between checkpoint crossings
8. Observation normalization uses named constants from OBS, not magic numbers
</verification>

<success_criteria>
- 9-ray cast returns normalized distances against track boundaries (AI-02)
- 14-value observation vector contains all required components (AI-03)
- Dense per-tick reward with continuous arc-length progress primary signal (AI-04)
- Four-tier penalties computed separately (AI-05)
- Default penalties smaller than progress rewards (AI-06)
- Reward weights configurable via RewardConfig type (AI-12)
- Per-component reward breakdown returned for logging (AI-13)
</success_criteria>

<output>
After completion, create `.planning/phases/04-gymnasium-environment-wrapper/04-01-SUMMARY.md`
</output>
</content>
</invoke>