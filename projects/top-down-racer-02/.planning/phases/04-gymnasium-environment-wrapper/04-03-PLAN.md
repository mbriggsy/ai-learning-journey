---
phase: 04-gymnasium-environment-wrapper
plan: 03
type: execute
wave: 3
depends_on: ["04-02"]
files_modified:
  - python/racer_env/__init__.py
  - python/racer_env/bridge_client.py
  - python/racer_env/config.py
  - python/racer_env/env.py
  - python/tests/test_env_checker.py
  - python/tests/test_random_agent.py
  - python/requirements.txt
autonomous: true
requirements: [AI-01, AI-07]

must_haves:
  truths:
    - "gymnasium.utils.env_checker.check_env(env) passes with no errors"
    - "A random-action agent completes 100 episodes without crashes or hangs"
    - "Bridge round-trip latency is under 0.5ms per step on localhost"
    - "Python Gymnasium wrapper connects to Node.js bridge server via WebSocket"
    - "RacerEnv has Box action space [steer, throttle, brake] and Box observation space (14,)"
  artifacts:
    - path: "python/racer_env/__init__.py"
      provides: "Package init exposing RacerEnv"
      contains: "from .env import RacerEnv"
    - path: "python/racer_env/bridge_client.py"
      provides: "Synchronous WebSocket RPC client"
      exports: ["BridgeClient"]
    - path: "python/racer_env/config.py"
      provides: "Config file loading"
      exports: ["load_config"]
    - path: "python/racer_env/env.py"
      provides: "Gymnasium-compatible RacerEnv"
      exports: ["RacerEnv"]
    - path: "python/tests/test_env_checker.py"
      provides: "check_env validation test"
    - path: "python/tests/test_random_agent.py"
      provides: "100-episode stability test"
    - path: "python/requirements.txt"
      provides: "Python dependency list"
  key_links:
    - from: "python/racer_env/env.py"
      to: "python/racer_env/bridge_client.py"
      via: "RacerEnv creates BridgeClient in __init__ and delegates step/reset"
      pattern: "BridgeClient"
    - from: "python/racer_env/bridge_client.py"
      to: "src/ai/bridge-server.ts"
      via: "WebSocket connection to ws://localhost:9876"
      pattern: "ws://localhost"
    - from: "python/racer_env/env.py"
      to: "python/racer_env/config.py"
      via: "loads reward config from JSON file"
      pattern: "load_config"
---

<objective>
Build the Python Gymnasium wrapper that connects to the TypeScript bridge server. Implement the WebSocket client, config loader, and Gymnasium environment class. Validate with env_checker and a 100-episode random agent.

Purpose: This completes the bridge -- Python RL training code can now interact with the TypeScript game engine through a standard Gymnasium interface. `check_env()` passing means stable-baselines3 (Phase 5) will work out of the box.

Output: Python package in `python/racer_env/` with Gymnasium-compatible `RacerEnv`, validation tests, and requirements file.
</objective>

<execution_context>
@~/.claude/commands/gsd/workflows/execute-plan.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-gymnasium-environment-wrapper/04-02-SUMMARY.md

<interfaces>
<!-- Bridge protocol (defined in Plan 02) -->

WebSocket RPC Protocol (JSON over ws://localhost:9876):

```
Client -> Server: { "type": "reset", "trackId": "track-01", "config": { "weights": {...}, "episode": {...} } }
Server -> Client: { "type": "reset_result", "observation": [14 floats], "info": {...} }

Client -> Server: { "type": "step", "action": [steer, throttle, brake] }
Server -> Client: { "type": "step_result", "observation": [14 floats], "reward": float, "terminated": bool, "truncated": bool, "info": {...} }

Client -> Server: { "type": "close" }
Server -> Client: { "type": "close_result" }
```

Action space: [steer: [-1,1], throttle: [0,1], brake: [0,1]]
Observation space: 14 floats, all in [-1, 1]

Info dict contains reward breakdown: progress, speed, wall, offTrack, backward, stillness

<!-- Config file format (python/ai-config.json) -->
```json
{
  "weights": {
    "progress": 1.0, "speedBonus": 0.1, "wallPenalty": -0.05,
    "offtrackPenalty": -0.02, "backwardPenalty": -0.1, "stillnessPenalty": -0.1
  },
  "episode": {
    "maxSteps": 3000, "stillnessTimeoutTicks": 180, "stillnessSpeedThreshold": 2.0
  }
}
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Python package with bridge client, config loader, and Gymnasium env</name>
  <files>
    python/requirements.txt
    python/racer_env/__init__.py
    python/racer_env/bridge_client.py
    python/racer_env/config.py
    python/racer_env/env.py
  </files>
  <action>
**Step 1: Create `python/requirements.txt`.**

```
gymnasium>=1.0.0
numpy>=1.26.0
websockets>=13.0
```

**Step 2: Create Python virtual environment and install dependencies.**

```bash
cd python
python -m venv .venv
.venv/Scripts/activate   # Windows
pip install -r requirements.txt
pip install pytest       # For running tests
```

Note: Use `python` (not `python3`) on Windows. Check which Python is available.

**Step 3: Create `python/racer_env/__init__.py`.**

```python
from .env import RacerEnv

__all__ = ["RacerEnv"]
```

**Step 4: Create `python/racer_env/config.py`.**

```python
import json
from pathlib import Path
from typing import Any

def load_config(config_path: str | Path = None) -> dict[str, Any]:
    """Load AI config from JSON file. Returns dict matching AiConfig schema."""
    if config_path is None:
        config_path = Path(__file__).parent.parent / "ai-config.json"
    config_path = Path(config_path)
    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")
    with open(config_path) as f:
        return json.load(f)
```

**Step 5: Create `python/racer_env/bridge_client.py`.**

This is the synchronous WebSocket RPC client. Key design:
- Uses `websockets.sync.client` (websockets v13+ has sync API) for simplicity
- NO asyncio needed -- the sync client wraps it internally
- Persistent connection created in `__init__`, reused across calls
- Retry-with-backoff on initial connection (max 5 attempts, starting at 100ms)
- Methods: `send_reset(config)`, `send_step(action)`, `send_close()`

```python
import json
import time
from websockets.sync.client import connect

class BridgeClient:
    def __init__(self, url: str = "ws://localhost:9876", max_retries: int = 5):
        self._url = url
        self._ws = None
        self._connect_with_retry(max_retries)

    def _connect_with_retry(self, max_retries: int):
        delay = 0.1  # 100ms initial
        for attempt in range(max_retries):
            try:
                self._ws = connect(self._url)
                return
            except Exception:
                if attempt == max_retries - 1:
                    raise
                time.sleep(delay)
                delay *= 2  # Exponential backoff

    def _send_recv(self, message: dict) -> dict:
        self._ws.send(json.dumps(message))
        response = self._ws.recv()
        result = json.loads(response)
        if result.get("type") == "error":
            raise RuntimeError(f"Bridge error: {result.get('message')}")
        return result

    def send_reset(self, config: dict = None, track_id: str = None) -> dict:
        msg = {"type": "reset"}
        if config is not None:
            msg["config"] = config
        if track_id is not None:
            msg["trackId"] = track_id
        return self._send_recv(msg)

    def send_step(self, action: list[float]) -> dict:
        return self._send_recv({"type": "step", "action": action})

    def send_close(self):
        try:
            self._send_recv({"type": "close"})
        except Exception:
            pass
        finally:
            if self._ws:
                self._ws.close()
                self._ws = None
```

**Step 6: Create `python/racer_env/env.py`.**

Gymnasium-compatible wrapper. Follow the EXACT Gymnasium custom env pattern.

```python
import gymnasium as gym
from gymnasium import spaces
import numpy as np
from .bridge_client import BridgeClient
from .config import load_config

class RacerEnv(gym.Env):
    metadata = {"render_modes": [], "render_fps": 60}

    def __init__(self, bridge_url="ws://localhost:9876", config_path=None, track_id="track-01"):
        super().__init__()

        # Action space: [steer, throttle, brake]
        self.action_space = spaces.Box(
            low=np.array([-1.0, 0.0, 0.0], dtype=np.float32),
            high=np.array([1.0, 1.0, 1.0], dtype=np.float32),
            dtype=np.float32,
        )

        # 14-value observation vector
        self.observation_space = spaces.Box(
            low=-1.0, high=1.0, shape=(14,), dtype=np.float32,
        )

        self._bridge = BridgeClient(bridge_url)
        self._config = load_config(config_path) if config_path else load_config()
        self._track_id = track_id

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        result = self._bridge.send_reset(
            config=self._config, track_id=self._track_id
        )
        obs = np.array(result["observation"], dtype=np.float32)
        info = result.get("info", {})
        return obs, info

    def step(self, action):
        action_list = action.tolist() if hasattr(action, 'tolist') else list(action)
        result = self._bridge.send_step(action_list)
        obs = np.array(result["observation"], dtype=np.float32)
        reward = float(result["reward"])
        terminated = bool(result["terminated"])
        truncated = bool(result["truncated"])
        info = result.get("info", {})
        return obs, reward, terminated, truncated, info

    def close(self):
        if self._bridge:
            self._bridge.send_close()
```

**CRITICAL details for check_env compliance:**
- `dtype=np.float32` in BOTH the Box space definition AND returned arrays
- `reset()` returns `(obs, info)` tuple (Gymnasium v1.0+ API)
- `step()` returns 5-tuple `(obs, reward, terminated, truncated, info)` (NOT old 4-tuple)
- `super().reset(seed=seed)` called in reset
- `observation_space` and `action_space` set in `__init__`
- Observation array shape matches `(14,)` exactly
- Action array shape matches `(3,)` exactly
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02 && python -c "from python.racer_env import RacerEnv; print('Import OK')" 2>/dev/null || cd python && .venv/Scripts/python -c "import sys; sys.path.insert(0,'.'); from racer_env import RacerEnv; print('Import OK')"</automated>
  </verify>
  <done>
    - Python package structure created: racer_env/__init__.py, env.py, bridge_client.py, config.py
    - requirements.txt lists gymnasium, numpy, websockets
    - Virtual environment created with dependencies installed
    - RacerEnv imports without errors
    - BridgeClient implements retry-with-backoff connection
    - Config loader reads python/ai-config.json
  </done>
</task>

<task type="auto">
  <name>Task 2: Create validation tests -- env_checker and random agent</name>
  <files>
    python/tests/test_env_checker.py
    python/tests/test_random_agent.py
    python/tests/__init__.py
  </files>
  <action>
**IMPORTANT:** These tests require the Node.js bridge server to be running. The test setup must:
1. Start the bridge server (`pnpm bridge`) as a subprocess
2. Wait for "listening" log message
3. Run the test
4. Kill the server on teardown

**Step 1: Create `python/tests/__init__.py`** -- empty file.

**Step 2: Create `python/tests/test_env_checker.py`.**

```python
"""Validate RacerEnv with Gymnasium's official env_checker (AI-01)."""
import subprocess
import sys
import time
import pytest
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent.parent

@pytest.fixture(scope="module")
def bridge_server():
    """Start the Node.js bridge server for testing."""
    proc = subprocess.Popen(
        ["npx", "tsx", "src/ai/bridge-server.ts"],
        cwd=str(PROJECT_ROOT),
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        shell=True,  # Required on Windows for npx
    )
    # Wait for server to be ready (look for "listening" in output)
    start = time.time()
    while time.time() - start < 10:
        line = proc.stdout.readline()
        if "listening" in line.lower():
            break
    else:
        proc.kill()
        raise RuntimeError("Bridge server failed to start within 10 seconds")

    yield proc

    proc.terminate()
    try:
        proc.wait(timeout=5)
    except subprocess.TimeoutExpired:
        proc.kill()

def test_env_checker(bridge_server):
    """check_env must pass with no errors (AI-01)."""
    from gymnasium.utils.env_checker import check_env
    sys.path.insert(0, str(PROJECT_ROOT / "python"))
    from racer_env import RacerEnv

    env = RacerEnv()
    try:
        check_env(env, skip_render_check=True)
    finally:
        env.close()
```

**Step 3: Create `python/tests/test_random_agent.py`.**

```python
"""100-episode random agent stability test."""
import subprocess
import sys
import time
import statistics
import pytest
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent.parent

@pytest.fixture(scope="module")
def bridge_server():
    """Start the Node.js bridge server for testing."""
    proc = subprocess.Popen(
        ["npx", "tsx", "src/ai/bridge-server.ts"],
        cwd=str(PROJECT_ROOT),
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        shell=True,
    )
    start = time.time()
    while time.time() - start < 10:
        line = proc.stdout.readline()
        if "listening" in line.lower():
            break
    else:
        proc.kill()
        raise RuntimeError("Bridge server failed to start")

    yield proc

    proc.terminate()
    try:
        proc.wait(timeout=5)
    except subprocess.TimeoutExpired:
        proc.kill()

def test_random_agent_100_episodes(bridge_server):
    """Random agent must complete 100 episodes without crashes or hangs."""
    sys.path.insert(0, str(PROJECT_ROOT / "python"))
    from racer_env import RacerEnv

    env = RacerEnv()
    episodes_completed = 0
    step_latencies = []

    try:
        for ep in range(100):
            obs, info = env.reset()
            assert obs.shape == (14,), f"Observation shape mismatch: {obs.shape}"
            assert obs.dtype.name == "float32", f"Observation dtype mismatch: {obs.dtype}"

            done = False
            steps = 0
            while not done:
                action = env.action_space.sample()
                start = time.perf_counter()
                obs, reward, terminated, truncated, info = env.step(action)
                elapsed = (time.perf_counter() - start) * 1000  # ms
                step_latencies.append(elapsed)

                done = terminated or truncated
                steps += 1

                # Safety: abort if episode runs too long (shouldn't happen with maxSteps)
                assert steps <= 5000, f"Episode {ep} exceeded 5000 steps"

            episodes_completed += 1
    finally:
        env.close()

    assert episodes_completed == 100, f"Only completed {episodes_completed}/100 episodes"

    # Latency check: median should be under 0.5ms
    median_latency = statistics.median(step_latencies)
    p99_latency = sorted(step_latencies)[int(len(step_latencies) * 0.99)]
    print(f"\nLatency stats: median={median_latency:.3f}ms, p99={p99_latency:.3f}ms")
    print(f"Total steps across 100 episodes: {len(step_latencies)}")

    # Report but don't hard-fail on latency (hardware-dependent)
    # The success criteria says "under 0.5ms" but we log it for manual review
    if median_latency > 0.5:
        print(f"WARNING: Median latency {median_latency:.3f}ms exceeds 0.5ms target")

def test_reward_components_logged(bridge_server):
    """Info dict must contain per-component reward breakdown (AI-13)."""
    sys.path.insert(0, str(PROJECT_ROOT / "python"))
    from racer_env import RacerEnv

    env = RacerEnv()
    try:
        obs, info = env.reset()
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)

        required_components = ["progress", "speed", "wall", "offTrack", "backward", "stillness"]
        for component in required_components:
            assert component in info, f"Missing reward component '{component}' in info dict"
    finally:
        env.close()
```

**Step 4: Extract bridge_server fixture to a conftest.py** to avoid duplication.

Create `python/tests/conftest.py`:

```python
import subprocess
import time
import pytest
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent.parent

@pytest.fixture(scope="session")
def bridge_server():
    """Start the Node.js bridge server for the entire test session."""
    proc = subprocess.Popen(
        ["npx", "tsx", "src/ai/bridge-server.ts"],
        cwd=str(PROJECT_ROOT),
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        shell=True,
    )
    start = time.time()
    while time.time() - start < 10:
        line = proc.stdout.readline()
        if "listening" in line.lower():
            break
    else:
        proc.kill()
        raise RuntimeError("Bridge server failed to start within 10 seconds")

    yield proc

    proc.terminate()
    try:
        proc.wait(timeout=5)
    except subprocess.TimeoutExpired:
        proc.kill()
```

Then simplify both test files to use the shared fixture (remove the duplicate `bridge_server` fixture from each file).

**Step 5: Run the tests.**

First, start the TypeScript build check:
```bash
cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02
pnpm exec tsc --noEmit
```

Then run the Python tests:
```bash
cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02/python
.venv/Scripts/python -m pytest tests/ -v
```

The conftest.py will auto-start the bridge server. Tests should:
1. `test_env_checker` -- passes check_env with no errors
2. `test_random_agent_100_episodes` -- completes 100 episodes
3. `test_reward_components_logged` -- info dict has all reward components

Also verify the full TypeScript test suite still passes:
```bash
cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02
pnpm test
```
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02/python && .venv/Scripts/python -m pytest tests/ -v --timeout=120</automated>
  </verify>
  <done>
    - check_env(RacerEnv) passes with no errors (AI-01 validated)
    - 100 episodes complete without crashes or hangs
    - Bridge latency reported (target: median under 0.5ms)
    - Info dict contains all 6 reward component keys (AI-13 validated)
    - TypeScript test suite still passes (no regressions)
  </done>
</task>

</tasks>

<verification>
1. `cd python && .venv/Scripts/python -m pytest tests/ -v --timeout=120` -- all Python tests pass
2. `cd .. && pnpm test` -- all TypeScript tests pass
3. `pnpm exec tsc --noEmit` -- no TypeScript errors
4. check_env passes (AI-01)
5. 100 episodes complete (stability)
6. Reward components in info dict (AI-13)
7. Config loaded from ai-config.json (AI-12)
</verification>

<success_criteria>
- gymnasium.utils.env_checker.check_env(env) passes with no errors (AI-01)
- A random-action agent completes 100 episodes without crashes or hangs
- Bridge round-trip latency reported (target: under 0.5ms median on localhost)
- Python Gymnasium wrapper connects to Node.js bridge via WebSocket (AI-07)
- Reward components logged per step via info dict (AI-13)
- Reward weights loaded from JSON config file (AI-12)
</success_criteria>

<output>
After completion, create `.planning/phases/04-gymnasium-environment-wrapper/04-03-SUMMARY.md`
</output>
