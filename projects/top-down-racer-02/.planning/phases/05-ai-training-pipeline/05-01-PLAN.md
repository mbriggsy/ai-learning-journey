---
phase: 05-ai-training-pipeline
plan: 01
type: standard
wave: 1
depends_on: []
files_modified:
  - python/requirements.txt
  - python/training/__init__.py
  - python/training/callbacks.py
  - python/training/benchmark.py
  - python/models/.gitkeep
  - python/logs/.gitkeep
autonomous: true
requirements: [AI-08, AI-09]

must_haves:
  truths:
    - "requirements.txt includes stable-baselines3[extra]>=2.4.0,<3 and torch>=2.3,<3 with upper bounds"
    - "RacerMetricsCallback uses record_mean() for per-step reward components (progress, speed, wall, offTrack, backward, stillness)"
    - "RacerMetricsCallback accumulates episode metrics and logs them in _on_rollout_end()"
    - "RacerMetricsCallback computes per-lap time (not cumulative) using step count deltas"
    - "Throughput benchmark script includes warmup phase and try/finally cleanup"
    - "models/ and logs/ directories exist with .gitkeep files"
  artifacts:
    - path: "python/requirements.txt"
      provides: "Updated dependency list with SB3 + PyTorch (upper-bounded)"
    - path: "python/training/__init__.py"
      provides: "Package init"
    - path: "python/training/callbacks.py"
      provides: "RacerMetricsCallback for custom TensorBoard logging"
      exports: ["RacerMetricsCallback"]
    - path: "python/training/benchmark.py"
      provides: "Throughput benchmark script for AI-08 validation"
    - path: "python/models/.gitkeep"
      provides: "Model checkpoint output directory"
    - path: "python/logs/.gitkeep"
      provides: "TensorBoard log output directory"
---

## Enhancement Summary

**Deepened on:** 2026-03-01
**Research agents used:** 10 (Python reviewer, Performance oracle, Architecture strategist, Code simplicity reviewer, Security sentinel, Pattern recognition specialist, Spec flow analyzer, SB3 best practices researcher, SB3 framework docs researcher, WebSocket RL performance researcher)

### Critical Bugs Found and Fixed
1. **`logger.record()` vs `record_mean()`** — Per-step reward logging used `record()` which keeps only the last value before dump. Changed to `record_mean()` which averages across the 2048-step rollout window.
2. **Cumulative lap time** — `stepCount / 60.0` gives cumulative episode time, not per-lap time. Fixed by tracking `_prev_step_counts` per env and computing the delta.
3. **Episode metrics logged in `_on_step()`** — Only the last episode's values survive to TensorBoard dump. Moved to accumulate during `_on_step()` and log aggregates in `_on_rollout_end()`.

### Key Improvements
1. Dependency version upper bounds for reproducibility
2. Warmup phase in benchmark for accurate measurements
3. try/finally cleanup in benchmark for robustness
4. Environment setup instructions (venv, Python version, GPU note)
5. Tick rate extracted as named constant

### Performance Insight (for future reference)
- Bridge server creates `new HeadlessEnv()` on every reset, rebuilding the track — should be fixed before training
- Redundant `distanceToTrackCenter` calls in engine (3x per tick) — future optimization target
- Actual training throughput will be 1500-2500 steps/sec (not 3000+) due to SB3 policy inference overhead

---

<objective>
Set up the training infrastructure: update Python dependencies, create the custom TensorBoard callback for racer-specific metrics, and build a throughput benchmark script. This establishes the foundation for the training scripts in Plan 02.

Purpose: AI-09 requires TensorBoard metrics beyond what SB3 auto-logs (lap time, completion rate, per-component rewards). AI-08 requires throughput validation before committing to long training runs.

Output: `python/training/` package with callbacks and benchmark, updated requirements.txt, output directories.
</objective>

<context>
@.planning/ROADMAP.md
@.planning/phases/05-ai-training-pipeline/05-RESEARCH.md
@.planning/phases/05-ai-training-pipeline/05-CONTEXT.md

<interfaces>
<!-- RacerEnv info dict structure (from Phase 4) -->
The info dict returned by RacerEnv.step() contains these keys:
- progress (float): per-tick progress reward component
- speed (float): speed bonus component
- wall (float): wall penalty component
- offTrack (float): off-track penalty component
- backward (float): backward penalty component
- stillness (float): stillness penalty component
- lap (int): current lap number (starts at 1, increments on lap completion)
- checkpoint (int): last checkpoint index crossed
- stepCount (int): total steps in current episode

The info dict returned by RacerEnv.reset() contains:
- lap (int): 1
- checkpoint (int): 0
- stepCount (int): 0

<!-- SB3 Logger behavior (from research) -->
SB3 Logger critical semantics:
- `logger.record(key, value)` — OVERWRITES previous value. Only the last value before dump() survives.
- `logger.record_mean(key, value)` — Running average. Accumulates across all calls between dump() invocations.
- `dump()` is called once per rollout iteration (every n_steps=2048 steps for PPO), NOT per step.
- After dump(), both name_to_value and name_to_count are cleared.
- `_on_rollout_end()` fires right before dump() — ideal place to log episode aggregates.

<!-- VecEnv auto-reset behavior -->
When done[i] == True:
- obs[i] is already the NEXT episode's first observation (auto-reset happened)
- infos[i] contains the TERMINAL step's info dict (your custom keys are correct here)
- infos[i]["terminal_observation"] has the real final observation
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update dependencies and create directory structure</name>
  <files>
    python/requirements.txt
    python/training/__init__.py
    python/models/.gitkeep
    python/logs/.gitkeep
  </files>
  <action>
**Step 0: Create/verify Python venv.**

Ensure the Python virtual environment exists and is activated:
```bash
cd python
python -m venv .venv
# Windows:
.venv\Scripts\activate
# Linux/macOS:
# source .venv/bin/activate
```

The `.gitignore` already has `python/.venv/` — this step ensures developers don't install ML packages (~2GB) into their global Python.

**Step 1: Update `python/requirements.txt`** to include SB3 and PyTorch with upper bounds.

```
# Requires Python 3.10-3.12 (PyTorch 2.3 compatibility)
# For NVIDIA GPU support, install PyTorch separately first:
#   pip install torch --index-url https://download.pytorch.org/whl/cu121
gymnasium>=1.0.0,<2
numpy>=1.26.0,<3
websocket-client>=1.7.0,<2
stable-baselines3[extra]>=2.4.0,<3
torch>=2.3,<3
```

The `[extra]` variant of SB3 bundles TensorBoard automatically.

### Research Insights

**Dependency Pinning:**
- Upper bounds (`<3`) prevent surprise major version upgrades that could break training reproducibility.
- For maximum reproducibility, generate a lock file after installation: `pip freeze > requirements.lock`
- The `~=` (compatible release) operator is even stricter but `>=,<` is more explicit.

**Python Version:**
- PyTorch 2.3 supports Python 3.8-3.12. Python 3.13 support arrived in PyTorch 2.5.
- SB3 >= 2.7.1 recommends Python >= 3.10.
- The comment in requirements.txt documents this constraint.

**CPU vs GPU:**
- `pip install torch` defaults to CPU-only on Windows.
- GPU training is 10-50x faster for policy gradient updates.
- The comment shows the CUDA install command for users with NVIDIA GPUs.

**Step 2: Create directory structure.**

Create `python/training/__init__.py` (empty package init).
Create `python/models/.gitkeep` (checkpoint output directory).
Create `python/logs/.gitkeep` (TensorBoard log directory).

**Step 3: Add output directories to `.gitignore`.**

Ensure these patterns are in the project `.gitignore`:
```
python/models/*.zip
python/models/*.pkl
python/logs/*/
python/.venv/
```

Model checkpoints (.zip) and VecNormalize stats (.pkl) should not be committed. The .gitkeep files ensure the directories exist. TensorBoard event files should not be committed.

**Step 4: Verify installation.**

Run `pip install -r python/requirements.txt` in the Python venv and verify SB3 imports:
```bash
cd python && python -c "from stable_baselines3 import PPO, SAC; print('SB3 OK')"
```

Also verify TensorBoard is launchable (prevents discovering it's missing after a long training run):
```bash
python -c "import tensorboard; print(f'TensorBoard {tensorboard.__version__} OK')"
```
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02/python && python -c "from stable_baselines3 import PPO, SAC; from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback; import tensorboard; print('SB3 + TensorBoard imports OK')"</automated>
  </verify>
  <done>
    - requirements.txt includes stable-baselines3[extra]>=2.4.0,<3 and torch>=2.3,<3
    - Python version constraint documented in requirements.txt comment
    - GPU install instructions documented in requirements.txt comment
    - python/training/ package exists with __init__.py
    - python/models/ and python/logs/ directories exist with .gitkeep
    - .gitignore updated for model/log artifacts
    - SB3 and TensorBoard import successfully in the Python environment
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement RacerMetricsCallback for TensorBoard</name>
  <files>
    python/training/callbacks.py
  </files>
  <action>
**Create `python/training/callbacks.py`** with `RacerMetricsCallback`.

This is a `BaseCallback` subclass that reads from the SB3 `infos` list on every step and logs racer-specific metrics to TensorBoard. It uses the correct SB3 logger methods based on metric type.

**Metrics to log:**

1. **Per-component reward breakdown** (averaged over rollout, under `reward/` namespace):
   - `reward/progress`, `reward/speed`, `reward/wall`, `reward/offTrack`, `reward/backward`, `reward/stillness`
   - Uses `self.logger.record_mean()` — averages across all steps in the rollout window (2048 steps for PPO).
   - `record_mean()` is required because `record()` keeps only the last value before `dump()`.

2. **Lap time** (per-lap, accumulated per rollout, under `racer/` namespace):
   - `racer/mean_lap_time_sec` — mean of all lap times in the rollout window
   - `racer/best_lap_time_sec` — fastest lap in the rollout window
   - Computed as `(stepCount - prev_step_count) / TICKS_PER_SECOND` when `lap` increments
   - Uses step count *delta*, not cumulative stepCount (which would give episode time, not lap time)

3. **Completion rate** (accumulated per rollout, under `racer/` namespace):
   - `racer/completion_rate` — fraction of episodes in the rollout where at least one lap was completed
   - `racer/episodes_this_rollout` — episodes seen in this rollout window

4. **Episode-level metrics are logged in `_on_rollout_end()`**, not `_on_step()`:
   - `_on_rollout_end()` fires right before `dump()`, so values are fresh
   - Prevents the "last-value-wins" problem of `record()` in a loop

**Implementation:**

```python
from __future__ import annotations

from typing import Any

from stable_baselines3.common.callbacks import BaseCallback


class RacerMetricsCallback(BaseCallback):
    """Log racer-specific metrics to TensorBoard.

    Per-step reward components use record_mean() to average across
    the entire rollout window (n_steps). Episode-level metrics
    (lap times, completion rate) are accumulated and logged once
    per rollout via _on_rollout_end().
    """

    TICKS_PER_SECOND: int = 60

    REWARD_KEYS: tuple[str, ...] = (
        "progress", "speed", "wall", "offTrack", "backward", "stillness",
    )

    def __init__(self, verbose: int = 0) -> None:
        super().__init__(verbose)
        self._prev_laps: list[int] = []
        self._prev_step_counts: list[int] = []
        # Accumulators cleared each rollout
        self._lap_times: list[float] = []
        self._episode_completions: list[float] = []  # 1.0 or 0.0

    def _on_training_start(self) -> None:
        num_envs: int = self.training_env.num_envs
        self._prev_laps = [1] * num_envs
        self._prev_step_counts = [0] * num_envs

    def _on_step(self) -> bool:
        infos: list[dict[str, Any]] = self.locals.get("infos", [])
        dones = self.locals.get("dones", [])

        for i, info in enumerate(infos):
            if i >= len(self._prev_laps):
                break  # Guard against mismatched env count

            # Per-step: accumulate with record_mean across all steps in rollout
            for key in self.REWARD_KEYS:
                if key in info:
                    self.logger.record_mean(f"reward/{key}", info[key])

            # Detect lap completion (lap number increased)
            current_lap: int = info.get("lap", 1)
            if current_lap > self._prev_laps[i]:
                step_count: int = info.get("stepCount", 0)
                # Per-lap time = delta from previous lap boundary, not cumulative
                lap_time = (step_count - self._prev_step_counts[i]) / self.TICKS_PER_SECOND
                self._lap_times.append(lap_time)
                self._prev_step_counts[i] = step_count
                if self.verbose >= 1:
                    print(f"  Lap completed! Time: {lap_time:.2f}s (env {i})")
            self._prev_laps[i] = current_lap

            # Track episode completion
            if i < len(dones) and dones[i]:
                completed = current_lap > 1
                self._episode_completions.append(float(completed))
                # Reset per-env state for next episode
                self._prev_laps[i] = 1
                self._prev_step_counts[i] = 0

        return True  # MUST return True explicitly (SB3 >= 2.2.1 enforces this)

    def _on_rollout_end(self) -> None:
        """Log episode-level aggregates just before dump() fires."""
        if self._episode_completions:
            n = len(self._episode_completions)
            rate = sum(self._episode_completions) / n
            self.logger.record("racer/completion_rate", rate)
            self.logger.record("racer/episodes_this_rollout", n)
            self._episode_completions.clear()

        if self._lap_times:
            import numpy as np
            self.logger.record("racer/mean_lap_time_sec", float(np.mean(self._lap_times)))
            self.logger.record("racer/best_lap_time_sec", min(self._lap_times))
            self._lap_times.clear()
```

### Research Insights

**`record()` vs `record_mean()` (CRITICAL):**
- SB3's `logger.record(key, value)` simply overwrites: `name_to_value[key] = value`. If called 2048 times between dumps, only the last value survives. This is a silent data corruption bug for per-step metrics.
- `logger.record_mean(key, value)` maintains a running average using Welford-style incremental computation. When `dump()` fires (once per n_steps rollout), the averaged value is written to TensorBoard.
- Both `name_to_value` and `name_to_count` are cleared after each `dump()`.
- Source: SB3 `stable_baselines3/common/logger.py`

**Dump Timing:**
- For PPO, `dump_logs()` is called every `log_interval` iterations (default 1), where each iteration = one rollout of `n_steps` steps.
- With n_steps=2048 and log_interval=1, dump fires every 2048 steps.
- `_on_rollout_end()` fires right before `dump()` — the ideal hook for logging episode aggregates.

**Per-Lap vs Cumulative Time:**
- `info["stepCount"]` is "total steps in current episode", not per-lap.
- Using cumulative `stepCount / 60.0` gives correct results for the first lap but wrong results for subsequent laps.
- Fix: Track `_prev_step_counts[i]` and compute `(stepCount - prev) / TICKS_PER_SECOND` on each lap boundary.
- With `maxSteps=3000` (50s), multi-lap episodes are unlikely in early training but become common as the agent improves.

**VecEnv Info Dict:**
- When `done[i] == True`, `infos[i]` contains the terminal step's info dict (custom keys like `lap`, `stepCount` are correct).
- The `obs[i]` is already the next episode's first observation (VecEnv auto-resets), but info is still terminal.
- The `"episode"` key (from Monitor wrapper) contains `{"r": reward, "l": length, "t": time}` — this is auto-logged by SB3 as `rollout/ep_rew_mean`.

**Multi-Env Loop:**
- The `for i, info in enumerate(infos)` pattern is NOT premature for single-env. SB3 always wraps in VecEnv, so `infos` is always a list (length 1 for DummyVecEnv). The loop costs nothing for n=1 and is the correct SB3 callback contract.

**Performance Impact:**
- 6x `record_mean()` calls per step add ~1-2 microseconds (dict operations). Bridge round-trip is 250+ microseconds. Logger overhead is under 1% of step time.
- TensorBoard write happens once per rollout (every 2048 steps) — amortized to ~1 microsecond per step.

**Edge Cases:**
- `_on_step()` returns `True` explicitly. SB3 >= 2.2.1 changed from `x is False` to `not x`, so a missing return (implicit `None`) stops training immediately.
- The bounds guard `if i >= len(self._prev_laps): break` prevents IndexError if env count is somehow mismatched.

**References:**
- https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html
- https://stable-baselines3.readthedocs.io/en/master/common/logger.html
- https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02/python && python -c "from training.callbacks import RacerMetricsCallback; cb = RacerMetricsCallback(verbose=1); print('Callback imports OK')"</automated>
  </verify>
  <done>
    - RacerMetricsCallback uses record_mean() for reward/progress, reward/speed, reward/wall, reward/offTrack, reward/backward, reward/stillness per step
    - RacerMetricsCallback detects lap completion and computes per-lap time using step count deltas
    - RacerMetricsCallback accumulates episode metrics and logs racer/completion_rate and racer/episodes_this_rollout in _on_rollout_end()
    - RacerMetricsCallback logs racer/mean_lap_time_sec and racer/best_lap_time_sec in _on_rollout_end()
    - Callback resets per-env state on episode boundaries
    - TICKS_PER_SECOND extracted as named class constant (not magic number)
    - Explicit return True for SB3 >= 2.2.1 compatibility
  </done>
</task>

<task type="auto">
  <name>Task 3: Create throughput benchmark script</name>
  <files>
    python/training/benchmark.py
  </files>
  <action>
**Create `python/training/benchmark.py`** — a standalone script to measure bridge throughput for AI-08 validation.

This script:
1. Connects to the running bridge server
2. Runs a warmup phase (excluded from measurements)
3. Runs N steps with random actions (no training overhead)
4. Reports steps/sec, mean latency, p50/p95/p99 latency
5. Exits with code 0 if >= threshold, code 1 if below, code 2 if bridge not running

**Usage:**
```bash
# Start bridge server in one terminal:
npx tsx src/ai/run-bridge.ts

# Run benchmark in another terminal:
cd python && python -m training.benchmark --steps 5000 --target 2000
```

**Implementation:**

```python
"""Throughput benchmark for the RacerEnv bridge (AI-08 validation).

Measures raw env.step() throughput (no SB3 policy inference overhead).
This represents the theoretical ceiling — actual training throughput will
be 20-40% lower due to PPO rollout collection overhead (policy forward
pass, tensor conversions, buffer storage).

Expected results:
- Raw bridge throughput: ~3000-4000 steps/sec (0.25-0.33ms/step)
- SB3 PPO training throughput: ~1500-2500 steps/sec (from time/fps TensorBoard metric)
"""
import argparse
import sys
import time
from pathlib import Path
from typing import Any

import numpy as np

# Add parent to path for racer_env imports
sys.path.insert(0, str(Path(__file__).parent.parent))
from racer_env import RacerEnv

WARMUP_STEPS = 200


def run_benchmark(
    steps: int = 5000,
    target: int = 2000,
    warmup: int = WARMUP_STEPS,
) -> dict[str, Any]:
    """Run throughput benchmark, return metrics dict.

    Includes a warmup phase to exclude JIT compilation, connection
    negotiation, and other one-time costs from measurements.
    """
    try:
        env = RacerEnv()
        obs, _ = env.reset()
    except (ConnectionRefusedError, OSError) as e:
        print(f"ERROR: Cannot connect to bridge server: {e}")
        print("Is the bridge running? Start it with: npx tsx src/ai/run-bridge.ts")
        sys.exit(2)

    try:
        # Warmup phase (not measured)
        for _ in range(warmup):
            action = env.action_space.sample()
            obs, _, terminated, truncated, _ = env.step(action)
            if terminated or truncated:
                obs, _ = env.reset()

        # Measurement phase
        latencies: list[float] = []
        t_start = time.perf_counter()
        for _ in range(steps):
            action = env.action_space.sample()
            t0 = time.perf_counter()
            obs, reward, terminated, truncated, info = env.step(action)
            latencies.append(time.perf_counter() - t0)
            if terminated or truncated:
                obs, _ = env.reset()
        t_end = time.perf_counter()
    finally:
        env.close()

    lat = np.array(latencies) * 1000  # Convert to ms
    elapsed = t_end - t_start
    sps = steps / elapsed

    return {
        "steps": steps,
        "warmup": warmup,
        "elapsed_sec": elapsed,
        "steps_per_sec": sps,
        "mean_ms": float(np.mean(lat)),
        "p50_ms": float(np.percentile(lat, 50)),
        "p95_ms": float(np.percentile(lat, 95)),
        "p99_ms": float(np.percentile(lat, 99)),
        "target": target,
        "passed": sps >= target,
    }


def main() -> None:
    parser = argparse.ArgumentParser(description="RacerEnv throughput benchmark")
    parser.add_argument("--steps", type=int, default=5000, help="Number of steps to measure")
    parser.add_argument("--target", type=int, default=2000, help="Target steps/sec (exit 1 if below)")
    parser.add_argument("--warmup", type=int, default=WARMUP_STEPS, help="Warmup steps (not measured)")
    args = parser.parse_args()

    print(f"Warming up ({args.warmup} steps)...")
    print(f"Measuring ({args.steps} steps)...")
    metrics = run_benchmark(args.steps, args.target, args.warmup)

    print(f"\nThroughput: {metrics['steps_per_sec']:.0f} steps/sec")
    print(f"Mean latency: {metrics['mean_ms']:.3f}ms")
    print(f"P50: {metrics['p50_ms']:.3f}ms | P95: {metrics['p95_ms']:.3f}ms | P99: {metrics['p99_ms']:.3f}ms")
    print(f"Target: {metrics['target']} steps/sec — {'PASS' if metrics['passed'] else 'FAIL'}")

    sys.exit(0 if metrics["passed"] else 1)


if __name__ == "__main__":
    main()
```

### Research Insights

**Benchmarking Methodology:**
- `time.perf_counter()` is the correct Python timer: sub-microsecond resolution on Windows via `QueryPerformanceCounter`, monotonic, minimal overhead.
- Both wall-clock throughput (`steps / total_elapsed`) and per-step latency percentiles are measured — wall-clock is the primary metric, percentiles are diagnostic.
- The default 200-step warmup excludes V8 JIT compilation, WebSocket connection establishment, and Python adaptive specialization from measurements.

**Throughput Expectations:**
- Raw bridge throughput (this benchmark): ~3000-4000 steps/sec based on Phase 4 median of 0.251ms/step.
- SB3 PPO training throughput: ~1500-2500 steps/sec. PPO adds per-step overhead: policy forward pass (~0.05-0.15ms), tensor/NumPy conversions (~0.02-0.05ms), plus periodic gradient update pauses (~1-2 seconds every n_steps=2048 steps).
- The 2000 steps/sec default target is calibrated for raw bridge throughput. Actual SB3 `time/fps` will be lower.
- The Plan 03 test threshold of 1500 steps/sec accounts for SB3 overhead and is well-calibrated.

**AI-08 Target Reconciliation:**
- AI-08 states "3000+ ticks/sec". The engine itself runs at 3000+ ticks/sec in-process (trivially fast).
- Through the WebSocket bridge with single-env, ~2000-4000 raw steps/sec is achievable (depends on step complexity).
- During SB3 training, effective throughput is 1500-2500 steps/sec. This is normal and expected for IPC-bridged single-env RL.
- For reference: most WebSocket/IPC-bridged RL environments achieve 500-5000 steps/sec single-env.

**Benchmark Note:**
- Random actions cause frequent crashes/resets, which is worst-case throughput. Actual training with a smarter policy will have fewer resets and potentially higher throughput.
- This benchmark does NOT include SB3 overhead. SB3's own `time/fps` TensorBoard metric is the ground-truth during training.

**Error Handling:**
- `ConnectionRefusedError` catch with clear error message prevents opaque WebSocket errors when bridge isn't running. Exit code 2 distinguishes from "bridge works but too slow" (exit 1).
- `try/finally` ensures `env.close()` is called even if the bridge crashes mid-benchmark, preventing dangling WebSocket connections.

**Quick Win (future):**
- Switching from `json` to `orjson` in `bridge_client.py` could yield 10-15% serialization speedup. This is a 2-line change but is out of scope for this plan.

**References:**
- https://superfastpython.com/benchmark-time-perf-counter/
- https://pyperf.readthedocs.io/en/latest/run_benchmark.html
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02/python && python -c "from training.benchmark import run_benchmark; print('Benchmark imports OK')"</automated>
  </verify>
  <done>
    - benchmark.py includes 200-step warmup phase before measurement
    - benchmark.py uses try/finally to ensure env.close() on error
    - benchmark.py catches ConnectionRefusedError with helpful message (exit code 2)
    - benchmark.py measures steps/sec through the real bridge connection
    - Reports mean, p50, p95, p99 latency
    - Exits with appropriate code based on target threshold (0=pass, 1=fail, 2=no connection)
    - run_benchmark() is importable for use in automated tests (Plan 03)
    - Supports --steps, --target, and --warmup CLI args
  </done>
</task>

</tasks>

<verification>
1. `pip install -r python/requirements.txt` succeeds in a Python 3.10-3.12 venv
2. `python -c "from stable_baselines3 import PPO, SAC"` imports without error
3. `python -c "import tensorboard"` imports without error
4. `python -c "from training.callbacks import RacerMetricsCallback"` imports without error
5. `python -c "from training.benchmark import run_benchmark"` imports without error
6. python/models/ and python/logs/ directories exist
7. .gitignore includes model/log artifact patterns
</verification>

<success_criteria>
- SB3 + PyTorch dependencies installable with upper bounds (AI-11 foundation)
- Custom callback uses record_mean() for per-step rewards and _on_rollout_end() for episode metrics (AI-09)
- Callback computes per-lap time (not cumulative episode time) using step count deltas (AI-09)
- Throughput benchmark includes warmup, error handling, and cleanup (AI-08 validation tool)
- Directory structure ready for training output
</success_criteria>

<output>
After completion, create `.planning/phases/05-ai-training-pipeline/05-01-SUMMARY.md`
</output>

---

## Appendix: Additional Research Findings

### A. Bridge Server Reset Performance (Priority 1 for future)

The performance oracle discovered that `bridge-server.ts` line 47 creates `new HeadlessEnv(trackId, config)` on every reset, which calls `buildTrack()` — an expensive multi-pass boundary computation with box filters, curvature analysis, and smoothing. During training with short episodes (180-3000 steps), this adds 5-10ms per reset.

**Fix (out of scope for Plan 01, but should be done before long training runs):**
```typescript
// In bridge-server.ts, reuse env if same track:
if (!env || trackId !== currentTrackId) {
  env = new HeadlessEnv(trackId, config);
  currentTrackId = trackId;
} else {
  env.reset();  // reuses the already-built track
}
```

### B. Engine Hot Path Redundancies (Future optimization)

`distanceToTrackCenter()` is called 3 times per tick:
- 2x from `stepWorld` via `getSurface()` (results discarded)
- 1x from `HeadlessEnv.step()` for progress tracking

Each call does ~750 spline evaluations (for a 1500-unit track). Over 2M timesteps, this is ~4.5B redundant spline evaluations. Refactoring to compute once and pass the result would reduce per-tick engine computation by 30-40%.

### C. Security Considerations

- **Dependency pinning**: Upper bounds added. Consider `pip freeze > requirements.lock` after known-good install.
- **Pickle risk**: SB3 uses pickle for model `.zip` and VecNormalize `.pkl` files. Never load models from untrusted sources.
- **WebSocket bridge**: Binds to 127.0.0.1 only (not 0.0.0.0). No authentication — acceptable for local-only development.
- **sys.path.insert**: Existing pattern from Phase 4 conftest.py. Consider `pyproject.toml` with `pip install -e .` to eliminate this across all scripts (recommended for Plan 02+).

### D. Recommended Callback Implementation for Plan 03 Tests

When testing the callback, note that `record_mean()` values are only retrievable via the logger's internal `name_to_value` dict. A unit test should:
1. Create a mock logger or use SB3's `configure()` to create a logger that captures values
2. Simulate multiple steps by calling `_on_step()` with crafted `self.locals`
3. Verify `name_to_value` contains the expected averaged values
4. Call `_on_rollout_end()` and verify episode aggregates
