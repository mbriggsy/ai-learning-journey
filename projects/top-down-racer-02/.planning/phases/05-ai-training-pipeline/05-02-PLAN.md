---
phase: 05-ai-training-pipeline
plan: 02
type: standard
wave: 2
depends_on: [05-01]
files_modified:
  - python/training/train_ppo.py
  - python/training/train_sac.py
  - python/training/evaluate.py
autonomous: true
requirements: [AI-10, AI-11]

must_haves:
  truths:
    - "train_ppo.py creates a PPO model with MlpPolicy, DummyVecEnv, VecNormalize, Monitor, and trains for a configurable number of timesteps"
    - "train_ppo.py saves model checkpoints periodically via CheckpointCallback and saves VecNormalize stats alongside"
    - "train_ppo.py supports --resume flag to load a checkpoint and continue training with reset_num_timesteps=False"
    - "train_sac.py provides equivalent SAC training as a fallback when PPO plateaus"
    - "evaluate.py loads a saved model + VecNormalize stats and runs inference episodes with deterministic actions"
    - "evaluate.py sets VecNormalize training=False and norm_reward=False at inference time"
    - "All scripts use argparse for CLI configuration (run name, timesteps, checkpoint path)"
    - "All scripts use try/finally to ensure vec_env.close() is called on exit or error"
    - "All scripts use pathlib-anchored paths relative to __file__, not CWD-relative strings"
    - "VecNormalize path derivation uses pathlib (not str.replace) and handles missing .zip extension"
    - "evaluate.py computes per-lap time using step count deltas (not cumulative stepCount / 60)"
    - "Training scripts catch KeyboardInterrupt and save an emergency checkpoint before exiting"
    - "Training scripts validate bridge connectivity before SB3 model initialization"
  artifacts:
    - path: "python/training/train_ppo.py"
      provides: "PPO training entry point with checkpointing and TensorBoard"
    - path: "python/training/train_sac.py"
      provides: "SAC training entry point (fallback if PPO plateaus)"
    - path: "python/training/evaluate.py"
      provides: "Model evaluation/inference script"
---

## Enhancement Summary

**Deepened on:** 2026-03-01
**Sections enhanced:** 3 tasks + new appendix
**Research agents used:** 10 (Python reviewer, Performance oracle, Architecture strategist, Security sentinel, Code simplicity reviewer, Pattern recognition specialist, SB3 best practices researcher, SB3 framework docs researcher, Spec flow analyzer, RL racing researcher)

### Critical Bugs Found and Fixed
1. **evaluate.py cumulative lap time bug** — `stepCount / 60.0` gives cumulative episode time, not per-lap time. Same bug that was fixed in callbacks.py during Plan 01 deepening. Fixed by tracking `prev_step_count` per episode.
2. **Missing `vec_env.close()`** — Neither training script nor evaluate.py calls `vec_env.close()`, leaking WebSocket connections on error or Ctrl+C. Fixed with `try/finally` blocks (matching benchmark.py pattern).
3. **`str.replace(".zip", ...)` path derivation** — Breaks if model path doesn't end in `.zip` (SB3's `model.save()` auto-appends it). Fixed with pathlib-based derivation.
4. **`--deterministic` flag is a no-op** — `action="store_true"` with `default=True` does nothing. Fixed with `BooleanOptionalAction`.

### Key Improvements
1. **try/finally env cleanup + KeyboardInterrupt save** — protects multi-hour training runs
2. **Bridge pre-flight check** — catches "bridge not running" before expensive SB3 initialization
3. **Pathlib-anchored output dirs** — `./logs/` and `./models/` resolved relative to `__file__`, not CWD
4. **Missing return type annotations** added to all public functions
5. **Consistent pathlib usage** (replacing `os.path` and f-string path concatenation)
6. **Pickle security warning comment** on all model-loading code paths

### Hyperparameter Insights (from RL Racing Research)
- **gamma=0.995** instead of 0.99 — gives ~200-step effective horizon (3.3s) for planning turns, vs ~100-step (1.7s) with 0.99
- **batch_size=128** for PPO — more stable gradients for continuous control, per rl-baselines3-zoo CarRacing config
- **SAC hyperparams updated** — `tau=0.02`, `learning_rate=7.3e-4`, `train_freq=8`, `gradient_steps=10` from Zoo CarRacing config
- **Reward tuning note** — current wall penalty (-0.002) is ~500x too small vs progress (1.0); research suggests -0.1 to -0.5

### Performance Insights
- **Estimated wall-clock:** 2M timesteps at ~2000 steps/sec = ~17 minutes. Fast enough for rapid reward tuning iteration.
- **SAC replay buffer:** ~140 MB for 1M entries with 14-dim obs. Trivially small.
- **Bridge server track rebuild on reset** adds 5-10ms per episode. Should be fixed before long training runs (see Plan 01 Appendix A).

### Architecture Decision: Keep Separate PPO/SAC Scripts
Multiple reviewers analyzed the duplication. The architecture strategist recommends keeping separate scripts because:
- PPO and SAC have fundamentally different training semantics (on-policy vs off-policy)
- Hyperparameter sets are structurally different (no shared schema)
- SAC may never be used (it's a fallback)
- This matches RL community convention (rl-baselines3-zoo, CleanRL)
If a third algorithm is added later, refactor to a unified `train.py --algo` at that point.

---

<objective>
Build the PPO and SAC training scripts and the model evaluation script. These are the core deliverables for AI-10 (checkpoint save/load) and AI-11 (PPO + SAC training via SB3).

Purpose: Enable the actual training loop — start training, monitor via TensorBoard, save checkpoints, resume from checkpoints, and evaluate trained models.

Output: Three Python scripts in `python/training/` that provide the complete training and evaluation pipeline.
</objective>

<context>
@.planning/ROADMAP.md
@.planning/phases/05-ai-training-pipeline/05-RESEARCH.md
@.planning/phases/05-ai-training-pipeline/05-CONTEXT.md
@.planning/phases/05-ai-training-pipeline/05-01-PLAN.md

<interfaces>
<!-- Dependencies from Plan 01 -->
From python/training/callbacks.py:
- RacerMetricsCallback(verbose=0) — BaseCallback subclass for TensorBoard metrics

From python/racer_env/env.py:
- RacerEnv(bridge_url="ws://localhost:9876", config_path=None, track_id="track-01")

From python/ai-config.json:
- Reward weights and episode config (read by RacerEnv on construction)

<!-- SB3 API (from research) -->
From stable_baselines3:
- PPO("MlpPolicy", env, verbose=1, tensorboard_log=LOG_DIR, **hyperparams)
- SAC("MlpPolicy", env, verbose=1, tensorboard_log=LOG_DIR, **hyperparams)
- model.learn(total_timesteps=N, callback=callbacks, tb_log_name=name)
- model.save(path) / PPO.load(path, env=env)
- model.predict(obs, deterministic=True) -> (action, _states)

From stable_baselines3.common:
- DummyVecEnv([make_env]) — vectorized env wrapper
- VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)
- VecNormalize.load(path, vec_env) — load saved normalization stats
- Monitor(env, log_dir) — episode reward auto-logging
- CheckpointCallback(save_freq, save_path, name_prefix, save_vecnormalize=True)
- EvalCallback(eval_env, best_model_save_path, eval_freq, n_eval_episodes)
- CallbackList([cb1, cb2, ...])
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PPO training script</name>
  <files>
    python/training/train_ppo.py
  </files>
  <action>
**Create `python/training/train_ppo.py`** — the primary training entry point.

**CLI interface:**
```bash
# Fresh training run:
python -m training.train_ppo --run-name ppo_run_1 --timesteps 2000000

# Resume from checkpoint:
python -m training.train_ppo --run-name ppo_run_1 --timesteps 1000000 --resume models/ppo_run_1_2000000_steps.zip
```

**Arguments:**
- `--run-name` (str, default: "ppo_run_1"): Name for this training run (used in log dirs, checkpoint filenames)
- `--timesteps` (int, default: 2_000_000): Total timesteps to train
- `--resume` (str, optional): Path to a saved model checkpoint to resume from
- `--checkpoint-freq` (int, default: 50_000): Save checkpoint every N steps
- `--track-id` (str, default: "track-01"): Track to train on
- `--bridge-url` (str, default: "ws://localhost:9876"): Bridge server URL

**Implementation:**

```python
"""PPO training script for the top-down racer AI.

Usage:
    cd python
    python -m training.train_ppo --run-name ppo_run_1 --timesteps 2000000
    python -m training.train_ppo --resume models/ppo_run_1_2000000_steps.zip --timesteps 1000000

Requires the bridge server to be running:
    npx tsx src/ai/run-bridge.ts
"""
import argparse
import sys
from collections.abc import Callable
from pathlib import Path

import gymnasium as gym

sys.path.insert(0, str(Path(__file__).parent.parent))

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList
from stable_baselines3.common.monitor import Monitor
from racer_env import RacerEnv
from training.callbacks import RacerMetricsCallback

# Resolve paths relative to the python/ directory, not CWD
PYTHON_DIR = Path(__file__).resolve().parent.parent
LOG_ROOT = PYTHON_DIR / "logs"
MODEL_DIR = PYTHON_DIR / "models"


def make_env(bridge_url: str, track_id: str, log_dir: str) -> Callable[[], gym.Env]:
    """Factory function for creating a monitored RacerEnv."""
    def _init() -> gym.Env:
        env = RacerEnv(bridge_url=bridge_url, track_id=track_id)
        env = Monitor(env, log_dir)
        return env
    return _init


def vecnorm_path_for(model_path: str) -> Path:
    """Derive VecNormalize stats path from a model path."""
    p = Path(model_path)
    stem = p.with_suffix("") if p.suffix == ".zip" else p
    return stem.parent / f"{stem.name}_vecnormalize.pkl"


def train(args: argparse.Namespace) -> None:
    log_dir = LOG_ROOT / args.run_name
    log_dir.mkdir(parents=True, exist_ok=True)
    MODEL_DIR.mkdir(exist_ok=True)

    # Pre-flight: verify bridge is reachable before expensive SB3 setup
    try:
        test_env = RacerEnv(bridge_url=args.bridge_url, track_id=args.track_id)
        test_env.reset()
        test_env.close()
    except (ConnectionError, OSError) as e:
        print(f"ERROR: Cannot connect to bridge server: {e}")
        print("Start the bridge first: npx tsx src/ai/run-bridge.ts")
        sys.exit(1)

    vec_env = DummyVecEnv([make_env(args.bridge_url, args.track_id, str(log_dir))])

    if args.resume:
        # Resume from checkpoint
        print(f"Resuming from {args.resume}")
        vnorm_path = vecnorm_path_for(args.resume)
        if vnorm_path.exists():
            vec_env = VecNormalize.load(str(vnorm_path), vec_env)
            vec_env.training = True
            vec_env.norm_reward = True
        else:
            # SECURITY: SB3 model files use pickle internally.
            # Never load models from untrusted sources.
            print(f"ERROR: VecNormalize stats not found at {vnorm_path}")
            print("A resumed model REQUIRES its original normalization stats.")
            print("Without them, observations will be on a different scale and the model will produce garbage actions.")
            vec_env.close()
            sys.exit(1)
        model = PPO.load(args.resume, env=vec_env)
    else:
        # Fresh training
        vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)
        model = PPO(
            "MlpPolicy",
            vec_env,
            verbose=1,
            tensorboard_log=str(log_dir),
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=128,
            n_epochs=10,
            gamma=0.995,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            vf_coef=0.5,
            max_grad_norm=0.5,
        )

    callbacks = CallbackList([
        CheckpointCallback(
            save_freq=args.checkpoint_freq,
            save_path=str(MODEL_DIR),
            name_prefix=args.run_name,
            save_vecnormalize=True,
        ),
        RacerMetricsCallback(verbose=1),
    ])

    try:
        model.learn(
            total_timesteps=args.timesteps,
            callback=callbacks,
            tb_log_name=args.run_name,
            reset_num_timesteps=args.resume is None,  # False when resuming
        )

        # Save final model and VecNormalize stats
        final_path = MODEL_DIR / f"{args.run_name}_final"
        model.save(str(final_path))
        vec_env.save(str(final_path.parent / f"{final_path.name}_vecnormalize.pkl"))
        print(f"\nTraining complete. Model saved to {final_path}.zip")
    except KeyboardInterrupt:
        # Save emergency checkpoint on Ctrl+C
        interrupted_path = MODEL_DIR / f"{args.run_name}_interrupted"
        print(f"\nInterrupted! Saving checkpoint to {interrupted_path}.zip ...")
        model.save(str(interrupted_path))
        vec_env.save(str(interrupted_path.parent / f"{interrupted_path.name}_vecnormalize.pkl"))
        print("Checkpoint saved. Resume with: --resume", str(interrupted_path) + ".zip")
    finally:
        vec_env.close()

    print(f"TensorBoard logs in {log_dir}/")
    print(f"\nView this run:  tensorboard --logdir {log_dir} --host localhost")
    print(f"Compare runs:   tensorboard --logdir {LOG_ROOT} --host localhost")


def main() -> None:
    parser = argparse.ArgumentParser(description="Train PPO agent on the top-down racer")
    parser.add_argument("--run-name", default="ppo_run_1", help="Name for this training run")
    parser.add_argument("--timesteps", type=int, default=2_000_000,
                        help="Additional timesteps to train (added to existing count when resuming)")
    parser.add_argument("--resume", type=str, default=None, help="Path to checkpoint .zip to resume from")
    parser.add_argument("--checkpoint-freq", type=int, default=50_000, help="Checkpoint save frequency (steps)")
    parser.add_argument("--track-id", default="track-01", help="Track ID to train on")
    parser.add_argument("--bridge-url", default="ws://localhost:9876", help="Bridge server WebSocket URL")
    args = parser.parse_args()

    if args.resume and not Path(args.resume).exists():
        # Also check with .zip appended (SB3 auto-appends .zip on save)
        if not Path(args.resume + ".zip").exists():
            parser.error(f"Resume checkpoint not found: {args.resume}")

    train(args)


if __name__ == "__main__":
    main()
```

**Key details:**
- Uses factory function `make_env()` for DummyVecEnv (avoids shared-instance anti-pattern).
- Monitor wrapper is inside the factory so episode stats are logged.
- `save_vecnormalize=True` in CheckpointCallback saves VecNormalize stats with each checkpoint.
- `reset_num_timesteps=False` when resuming preserves TensorBoard x-axis continuity.
- VecNormalize resume: loads stats from `.pkl` file adjacent to the checkpoint `.zip`.

**PPO hyperparameters (from research):**
- `batch_size=128` — more stable gradients for continuous control, per rl-baselines3-zoo CarRacing config.
- `gamma=0.995` — ~200-step effective horizon (3.3s at 60Hz) for planning turns. 0.99 only gives ~100-step horizon (1.7s), which may be too short for the agent to plan through corners.
- `ent_coef=0.01` — small entropy bonus for exploration. Research (Nature 2025) confirms this works well for racing.
- `n_steps=2048` — standard for single-env PPO. Rollout collection takes ~1 second, gradient update ~30-50ms. Appropriate ratio.
- These are starting points; reward tuning iterations may adjust them.

### Research Insights

**Best Practices:**
- Bridge connectivity pre-flight check prevents expensive SB3 initialization (model + optimizer + buffers) before discovering the bridge isn't running.
- Missing VecNormalize stats on resume is now an error (not a warning), because a model trained with normalized observations will produce garbage with fresh (mean=0, var=1) stats.
- `try/finally vec_env.close()` prevents WebSocket connection leaks. Matches the pattern already used in benchmark.py.
- `KeyboardInterrupt` handler saves an emergency checkpoint. With checkpoint_freq=50K at ~2000 steps/sec, Ctrl+C could otherwise lose up to 25 seconds of training.
- Output paths anchored to `PYTHON_DIR = Path(__file__).resolve().parent.parent` so scripts work regardless of CWD.

**Performance Considerations:**
- With single-env at ~2000 effective steps/sec, 2M timesteps takes ~17 minutes. Fast enough for rapid reward tuning iteration.
- `n_steps=2048` with `batch_size=128` gives 16 minibatches per epoch, 160 total gradient steps per rollout. For a ~5K parameter MLP, this is well-calibrated.
- Bridge server track rebuild on every reset adds 5-10ms overhead. Should be fixed (see Plan 01 Appendix A) before long training runs.

**TensorBoard Diagnostics (what to watch):**
- `train/approx_kl` should stay below 0.03. Sustained spikes above 0.05 = reduce learning_rate.
- `train/explained_variance` should climb toward 0.5-0.9 during training. Stuck near 0 = value function not learning.
- `train/entropy_loss` should decrease gradually. Rapid collapse to 0 = premature policy collapse, increase `ent_coef`.
- `train/clip_fraction` should be 0.05-0.2. Near 0 = clip_range too wide. Near 1 = policy changing too fast.
- Custom `racer/completion_rate` and `racer/mean_lap_time_sec` are the true success metrics.

**Edge Cases:**
- `--timesteps` on resume means "train N more steps" (SB3 behavior with `reset_num_timesteps=False`), not "train until total reaches N". Help text clarified.
- Concurrent training sessions with the same `--run-name` will collide on checkpoint files. Run names must be unique.
- Two Ctrl+C presses exit immediately (Python default). Only the first triggers the save handler.

**Security:**
- SB3 model `.zip` files use pickle internally. Never load models from untrusted sources -- a crafted file can execute arbitrary code.
- TensorBoard binds to `--host localhost` (explicit in print output) to prevent network exposure.

**References:**
- https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html
- https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml
- https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html
- https://www.nature.com/articles/s41598-025-27702-6 (Racing RL reward design, 2025)
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02/python && python -c "from training.train_ppo import make_env, train; print('train_ppo imports OK')"</automated>
  </verify>
  <done>
    - train_ppo.py supports fresh training and --resume from checkpoint
    - Uses DummyVecEnv + VecNormalize + Monitor wrapper chain
    - CheckpointCallback saves model + VecNormalize stats every checkpoint_freq steps
    - RacerMetricsCallback logs custom racer metrics to TensorBoard
    - reset_num_timesteps=False when resuming (preserves TensorBoard continuity)
    - Final model + VecNormalize stats saved at training end
    - All hyperparameters use SB3 standard defaults for continuous control
  </done>
</task>

<task type="auto">
  <name>Task 2: Create SAC training script</name>
  <files>
    python/training/train_sac.py
  </files>
  <action>
**Create `python/training/train_sac.py`** — the fallback training script for when PPO plateaus.

The structure mirrors train_ppo.py with SAC-specific hyperparameters. Key differences from PPO:
- SAC is off-policy (uses a replay buffer)
- `ent_coef="auto"` — entropy auto-tuned (one less hyperparameter)
- `buffer_size=1_000_000` — replay buffer (PPO doesn't have one)
- `learning_starts=1000` — collect random data before training begins
- `train_freq=1` — train on every step
- `batch_size=256` — larger batch for off-policy learning

**CLI interface (same pattern as PPO):**
```bash
python -m training.train_sac --run-name sac_run_1 --timesteps 2000000
python -m training.train_sac --run-name sac_run_1 --timesteps 1000000 --resume models/sac_run_1_final.zip
```

**Implementation:**

Same structure as train_ppo.py (bridge pre-flight, pathlib-anchored paths, try/finally cleanup, KeyboardInterrupt handler, VecNormalize error on resume), with these changes:

```python
from stable_baselines3 import SAC

# In the fresh training branch:
model = SAC(
    "MlpPolicy",
    vec_env,
    verbose=1,
    tensorboard_log=str(log_dir),
    learning_rate=7.3e-4,
    buffer_size=300_000,
    learning_starts=10_000,
    batch_size=256,
    tau=0.02,
    gamma=0.99,
    train_freq=8,
    gradient_steps=10,
    ent_coef="auto",
    use_sde=True,
)

# In the resume branch:
model = SAC.load(args.resume, env=vec_env)
```

**Important:** VecNormalize with `norm_reward=True` is especially important for SAC because the entropy auto-tuning assumes rewards on a roughly unit scale (research Pitfall 5).

### Research Insights

**SAC Hyperparameters (updated from rl-baselines3-zoo CarRacing config):**
- `learning_rate=7.3e-4` — notably higher than PPO's 3e-4. SAC's off-policy updates tolerate higher LR.
- `buffer_size=300_000` — sufficient for 14-dim obs (memory: ~42 MB for obs + next_obs + actions + rewards + dones). The original 1M was overkill; 300K holds ~2.5 minutes of experience at 2000 steps/sec.
- `learning_starts=10_000` — random exploration before training. At 3000 max steps/episode, this is ~3-4 random episodes. Fills the buffer with diverse initial data.
- `train_freq=8` with `gradient_steps=10` — collects 8 env steps, then does 10 gradient updates. More efficient than the default (train_freq=1, gradient_steps=1) because it batches env interaction.
- `tau=0.02` — faster soft target updates (was 0.005). Racing requires quicker adaptation to value changes.
- `use_sde=True` — State Dependent Exploration produces smoother, more physically coherent exploration than i.i.d. Gaussian noise. Particularly beneficial for vehicle control where correlated actions across timesteps make sense.

**When to Switch from PPO to SAC:**
Consider SAC if `rollout/ep_rew_mean` has not increased for 500K+ steps AND `racer/completion_rate` remains below 0.5 after 1M+ total PPO steps. Always try reward weight tuning first — research consistently shows reward design matters more than algorithm choice.

**SAC Cannot Warm-Start from PPO:**
PPO and SAC have different network architectures (PPO: shared actor-critic; SAC: separate actor + twin critics + target networks). Weights cannot be transferred. SAC training always starts from scratch.

**SAC Memory Usage:**
| Component | Size |
|---|---|
| Replay buffer (300K x 14-dim obs) | ~42 MB |
| Policy networks (actor + 2 critics + 2 targets) | ~60 KB |
| PyTorch overhead | ~200-400 MB |
| **Total** | **~300-500 MB** |

**References:**
- https://stable-baselines3.readthedocs.io/en/master/modules/sac.html
- https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/sac.yml
- https://www.findingtheta.com/blog/solving-gymnasiums-car-racing-with-reinforcement-learning
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02/python && python -c "from training.train_sac import make_env, train; print('train_sac imports OK')"</automated>
  </verify>
  <done>
    - train_sac.py mirrors train_ppo.py structure with SAC-specific hyperparameters
    - Uses ent_coef="auto" for entropy auto-tuning
    - Includes replay buffer configuration (buffer_size, learning_starts)
    - Supports --resume with same VecNormalize load pattern
    - VecNormalize norm_reward=True ensures SAC entropy tuning stability
  </done>
</task>

<task type="auto">
  <name>Task 3: Create model evaluation script</name>
  <files>
    python/training/evaluate.py
  </files>
  <action>
**Create `python/training/evaluate.py`** — loads a trained model and runs inference episodes.

This script validates AI-10 (checkpoint loading) and is needed for Phase 6 (deploy model to browser). It also serves as a manual verification tool during reward tuning.

**CLI interface:**
```bash
# Run 10 evaluation episodes with a trained model:
python -m training.evaluate --model models/ppo_run_1_final.zip --episodes 10

# Specify normalization stats (auto-detected by convention):
python -m training.evaluate --model models/ppo_run_1_final.zip --vecnorm models/ppo_run_1_final_vecnormalize.pkl
```

**Arguments:**
- `--model` (str, required): Path to saved model .zip
- `--vecnorm` (str, optional): Path to VecNormalize .pkl stats (auto-detected from model path if not provided)
- `--episodes` (int, default: 10): Number of episodes to run
- `--track-id` (str, default: "track-01"): Track to evaluate on
- `--bridge-url` (str, default: "ws://localhost:9876"): Bridge server URL
- `--deterministic` (flag, default: True): Use deterministic actions

**Implementation:**

```python
"""Evaluate a trained model by running inference episodes.

Usage:
    cd python
    python -m training.evaluate --model models/ppo_run_1_final.zip --episodes 10

Requires the bridge server to be running:
    npx tsx src/ai/run-bridge.ts
"""
import argparse
import sys
from collections.abc import Callable
from pathlib import Path

import gymnasium as gym

sys.path.insert(0, str(Path(__file__).parent.parent))

from stable_baselines3 import PPO, SAC
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from racer_env import RacerEnv

TICKS_PER_SECOND: int = 60


def make_env(bridge_url: str, track_id: str) -> Callable[[], gym.Env]:
    """Factory function for creating a RacerEnv (no Monitor needed for evaluation)."""
    def _init() -> gym.Env:
        return RacerEnv(bridge_url=bridge_url, track_id=track_id)
    return _init


def vecnorm_path_for(model_path: str) -> Path:
    """Derive VecNormalize stats path from a model path."""
    p = Path(model_path)
    stem = p.with_suffix("") if p.suffix == ".zip" else p
    return stem.parent / f"{stem.name}_vecnormalize.pkl"


def load_model(model_path: str, vec_env: VecNormalize) -> PPO | SAC:
    """Load model, auto-detecting algorithm from the saved file.

    SECURITY: SB3 model files use pickle internally.
    Never load models from untrusted sources.
    """
    try:
        return PPO.load(model_path, env=vec_env)
    except (ValueError, KeyError) as e:
        if "class" not in str(e).lower() and "ppo" not in str(e).lower():
            raise  # Re-raise unexpected errors
        return SAC.load(model_path, env=vec_env)


def evaluate(args: argparse.Namespace) -> None:
    vec_env = DummyVecEnv([make_env(args.bridge_url, args.track_id)])

    # Load VecNormalize stats
    vnorm_path = Path(args.vecnorm) if args.vecnorm else vecnorm_path_for(args.model)
    if vnorm_path.exists():
        vec_env = VecNormalize.load(str(vnorm_path), vec_env)
    else:
        print(f"Warning: No VecNormalize stats at {vnorm_path}, using unnormalized observations")
        vec_env = VecNormalize(vec_env, norm_obs=False, norm_reward=False)

    # CRITICAL: Disable training mode for inference
    vec_env.training = False
    vec_env.norm_reward = False

    model = load_model(args.model, vec_env)

    print(f"Evaluating {args.model} on {args.track_id} for {args.episodes} episodes")
    print(f"Deterministic: {args.deterministic}\n")

    lap_times: list[float] = []
    rewards: list[float] = []

    try:
        for ep in range(args.episodes):
            obs = vec_env.reset()
            ep_reward = 0.0
            done = False
            prev_lap = 1
            prev_step_count = 0  # Track step count at last lap boundary

            while not done:
                action, _ = model.predict(obs, deterministic=args.deterministic)
                obs, reward, dones, infos = vec_env.step(action)
                ep_reward += reward[0]
                done = dones[0]
                info = infos[0]

                # Track lap completion — use step count DELTA for per-lap time
                current_lap = info.get("lap", 1)
                if current_lap > prev_lap:
                    step_count = info.get("stepCount", 0)
                    lt = (step_count - prev_step_count) / TICKS_PER_SECOND
                    lap_times.append(lt)
                    prev_step_count = step_count
                    print(f"  Episode {ep+1}: Lap completed in {lt:.2f}s")
                prev_lap = current_lap

            rewards.append(ep_reward)
            steps = info.get("stepCount", 0)
            print(f"  Episode {ep+1}: reward={ep_reward:.2f}, steps={steps}, laps={info.get('lap', 1)-1}")
    finally:
        vec_env.close()

    print(f"\n--- Summary ({args.episodes} episodes) ---")
    print(f"Mean reward: {sum(rewards)/len(rewards):.2f}")
    if lap_times:
        print(f"Laps completed: {len(lap_times)}")
        print(f"Mean lap time: {sum(lap_times)/len(lap_times):.2f}s")
        print(f"Best lap time: {min(lap_times):.2f}s")
    else:
        print("No laps completed.")
    print(f"\nNote: Visual evaluation requires loading the model in the browser (Phase 6).")


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Evaluate a trained racer model (requires bridge server running)")
    parser.add_argument("--model", type=str, required=True, help="Path to saved model .zip")
    parser.add_argument("--vecnorm", type=str, default=None, help="Path to VecNormalize .pkl stats")
    parser.add_argument("--episodes", type=int, default=10, help="Number of evaluation episodes")
    parser.add_argument("--track-id", default="track-01", help="Track ID to evaluate on")
    parser.add_argument("--bridge-url", default="ws://localhost:9876", help="Bridge server WebSocket URL")
    parser.add_argument("--deterministic", action=argparse.BooleanOptionalAction, default=True,
                        help="Use deterministic actions (--no-deterministic for stochastic)")
    args = parser.parse_args()
    evaluate(args)


if __name__ == "__main__":
    main()
```

**Key details:**
- `vec_env.training = False` — stops updating normalization statistics at inference.
- `vec_env.norm_reward = False` — raw rewards for readable output.
- Auto-detects VecNormalize path from model path convention using pathlib.
- Auto-detects algorithm (PPO vs SAC) via try/except on load, with narrowed exception clause.
- Reports per-episode rewards, steps, laps, and aggregate statistics.
- Deterministic mode by default (consistent behavior for evaluation).

### Research Insights

**Bugs Fixed:**
- **Per-lap time calculation:** The original `stepCount / 60.0` gave cumulative episode time, not per-lap time. This is the exact same bug that was identified and fixed in `callbacks.py` during Plan 01 deepening. Now uses step count deltas (`(stepCount - prev_step_count) / TICKS_PER_SECOND`).
- **Lambda closure replaced with factory function:** The original `DummyVecEnv([lambda: RacerEnv(...)])` is a late-binding closure footgun. If copied into a loop for multi-env, all envs share the last iteration's values. Factory function is safer and consistent with training scripts.
- **`--deterministic` flag fixed:** `action="store_true"` with `default=True` is a no-op. Replaced with `BooleanOptionalAction` which produces `--deterministic` / `--no-deterministic` automatically.
- **try/finally added for env cleanup:** Matches the pattern in benchmark.py. Prevents WebSocket connection leak if bridge disconnects mid-evaluation.
- **Narrowed exception clause:** The try/except for algorithm detection now re-raises unexpected `ValueError`s instead of silently falling through to SAC.

**Performance Note:**
- For PPO, `deterministic=True` returns the Gaussian distribution mean (no noise).
- For SAC, `deterministic=True` returns the squashed Gaussian mean (tanh applied, no noise).
- Both produce consistent, reproducible actions suitable for evaluation.
- Stochastic mode (`--no-deterministic`) is useful for seeing exploration behavior during debugging.

**Visual Evaluation Gap:**
- `evaluate.py` reports metrics only (rewards, steps, lap times). There is no way to visually watch the AI drive.
- Visual evaluation requires loading the model in the browser (Phase 6 scope).
- For reward tuning, watch for pathological metrics: high reward but 0 laps completed = reward hacking. Many laps but high wall contact = wall-hugging.

**References:**
- https://stable-baselines3.readthedocs.io/en/master/guide/save_format.html
- https://stable-baselines3.readthedocs.io/en/master/guide/examples.html
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02/python && python -c "from training.evaluate import load_model, evaluate; print('evaluate imports OK')"</automated>
  </verify>
  <done>
    - evaluate.py loads saved model + VecNormalize stats
    - Sets training=False, norm_reward=False at inference (prevents stats corruption)
    - Auto-detects VecNormalize path from model path convention
    - Auto-detects algorithm type (PPO or SAC)
    - Reports per-episode rewards, steps, laps; plus aggregate mean/best lap times
    - Supports deterministic and stochastic action modes
    - Validates the full AI-10 checkpoint save→load→inference pipeline
  </done>
</task>

</tasks>

<verification>
1. `python -c "from training.train_ppo import train"` imports without error
2. `python -c "from training.train_sac import train"` imports without error
3. `python -c "from training.evaluate import evaluate"` imports without error
4. train_ppo.py --help shows all expected arguments
5. train_sac.py --help shows all expected arguments
6. evaluate.py --help shows all expected arguments
7. Code uses reset_num_timesteps=False on resume (verified by reading source)
8. evaluate.py sets vec_env.training=False (verified by reading source)
</verification>

<success_criteria>
- PPO training with MlpPolicy, DummyVecEnv, VecNormalize, Monitor, TensorBoard logging (AI-11)
- SAC training as fallback with appropriate off-policy hyperparameters (AI-11)
- Model checkpoint saving via CheckpointCallback + final save (AI-10)
- Model checkpoint loading with VecNormalize stats for resume and inference (AI-10)
- TensorBoard integration via tensorboard_log parameter + RacerMetricsCallback (AI-09)
</success_criteria>

<output>
After completion, create `.planning/phases/05-ai-training-pipeline/05-02-SUMMARY.md`
</output>

---

## Appendix: Research Findings

### A. SB3 API Deep Dive

**CheckpointCallback Naming Convention:**
SB3's `CheckpointCallback` names files as `{name_prefix}_{num_timesteps}_steps.zip`. The `num_timesteps` value is cumulative across resumes (since `reset_num_timesteps=False` is used). Example sequence:
```
ppo_run_1_500000_steps.zip
ppo_run_1_1000000_steps.zip   # after resume at 500K + 500K more
```

**`reset_num_timesteps=False` Behavior:**
- Preserves TensorBoard x-axis (total timesteps) across resumes
- Preserves learning rate schedule position (if using schedule)
- Preserves the `num_timesteps` counter used by CheckpointCallback
- Does NOT preserve the replay buffer for SAC (requires separate `save_replay_buffer()` / `load_replay_buffer()`)

**Algorithm Detection from Saved Model:**
SB3 `.zip` files contain a `data` dict with a `"policy_class"` key. However, `PPO.load()` vs `SAC.load()` validate the internal class metadata and raise `ValueError` on mismatch — which is what our `load_model()` try/except leverages.

### B. Reward Tuning Guide

The current `ai-config.json` penalties may be too small to drive learning:
- `wallPenalty: -0.002` — at 60 Hz, continuous wall contact costs -0.12/sec. May be too weak.
- `offTrackPenalty: -0.001` — even weaker than wall penalty.
- `stillnessPenalty: -0.001` — combined with 180-tick timeout, total cost is only -0.18. May not prevent parking.

**Recommended tuning sequence** (after scripts are built):
1. Train PPO for 500K steps with default weights → observe TensorBoard
2. If `racer/wall_contacts` is high → increase `wallPenalty` to -0.01
3. If `racer/completion_rate` is 0 after 1M steps → increase `progress` to 2.0
4. If agent drives in circles → add backward penalty (-0.01)
5. If agent parks → decrease stillness timeout to 90 ticks

### C. Training Duration Estimates

Based on the Phase 4 benchmark (~3980 steps/sec, median 0.251ms latency):

| Timesteps | Wall Clock | TensorBoard Points | Notes |
|---|---|---|---|
| 500K | ~2.1 min | ~167 episodes | Quick sanity check |
| 1M | ~4.2 min | ~333 episodes | Minimum for reward tuning |
| 2M | ~8.4 min | ~667 episodes | PPO should show clear learning |
| 5M | ~21 min | ~1667 episodes | Expected convergence range |
| 10M | ~42 min | ~3333 episodes | Diminishing returns expected |

Note: SAC is slower per step due to gradient_steps=10 per 8 env steps (~60% slower wall clock than PPO for the same timestep count).

### D. Test File Compatibility Warning

Plan 03 test files reference a `FakeLogger` that only implements `record()`, not `record_mean()`. Since Plan 01 deepening changed the callback to use `record_mean()` for per-step metrics, the test files in Plan 03 will need updating. Specifically:
- `test_racer_callback.py`: `FakeLogger` needs `record_mean()` method
- Key assertions may need to check different metric names

This should be flagged when Plan 03 begins execution.

### E. SB3 Known Issues

1. **numpy 2.0 compatibility:** SB3 ≤ 2.3.x may have issues with numpy 2.0+. Pin `numpy<2.0` in requirements.txt if errors occur during training.
2. **VecNormalize pickling (GitHub #2101):** Older SB3 versions had issues with VecNormalize `.pkl` files across Python versions. Use the same Python version for training and evaluation.
3. **Pickle security:** All SB3 model files (`.zip`) and VecNormalize stats (`.pkl`) use Python pickle internally. Never load from untrusted sources — a crafted file can execute arbitrary code.

### F. Future Optimizations (Out of Scope)

| Priority | Optimization | Expected Impact |
|---|---|---|
| P0 | Reward weight tuning | Required for competent driving |
| P1 | `SubprocVecEnv` (multi-env) | 2-4x throughput with 4 bridge instances |
| P2 | Frame stacking (last 3 obs) | Better temporal reasoning |
| P3 | Curriculum learning (straight → curves) | Faster initial convergence |
| P4 | LSTM/GRU policy | Memory for corner sequences |
| P5 | Observation space expansion (track curvature) | Look-ahead ability |
| P6 | Domain randomization (physics noise) | Sim-to-real robustness |

### G. Reference Projects

| Project | Relevance |
|---|---|
| [rl-baselines3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo) | Tuned hyperparameters for CarRacing-v2, training scripts structure |
| [tmrl](https://github.com/trackmania-rl/tmrl) | Real racing game RL (TrackMania), reward design patterns |
| [SB3 docs: RL Tips](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html) | Official recommendations for continuous control |
| [Racing RL reward design (2025)](https://www.nature.com/articles/s41598-025-27702-6) | Academic paper on reward shaping for racing |
