---
phase: 05-ai-training-pipeline
plan: 03
type: tdd
wave: 3
depends_on: [05-01, 05-02]
files_modified:
  - python/tests/test_throughput.py
  - python/tests/test_callbacks.py
  - python/tests/test_checkpoint.py
autonomous: true
requirements: [AI-08, AI-09, AI-10]

must_haves:
  truths:
    - "Throughput test measures actual bridge steps/sec and asserts >= target threshold"
    - "Callback unit test verifies RacerMetricsCallback logs expected metric keys via record_mean() and record() without requiring a bridge connection"
    - "Callback tests call _on_rollout_end() before asserting episode-level metrics (matching the real SB3 lifecycle)"
    - "Checkpoint integration test verifies save→load→predict pipeline produces valid actions"
    - "Checkpoint test verifies VecNormalize running statistics survive the save/load round-trip"
    - "All existing Phase 4 tests continue to pass (no regressions)"
  artifacts:
    - path: "python/tests/test_throughput.py"
      provides: "Bridge throughput benchmark test for AI-08"
    - path: "python/tests/test_callbacks.py"
      provides: "Callback unit test for AI-09 (offline, no bridge needed)"
    - path: "python/tests/test_checkpoint.py"
      provides: "Checkpoint save/load integration test for AI-10"
---

## Enhancement Summary

**Deepened on:** 2026-03-01
**Sections enhanced:** 3 tasks + new appendix
**Research agents used:** 10 (Python reviewer, Performance oracle, Architecture strategist, Code simplicity reviewer, Security sentinel, Pattern recognition specialist, Spec flow analyzer, SB3 framework docs researcher, RL testing best practices researcher, Pytest patterns researcher)

### Critical Bugs Found and Fixed
1. **FakeLogger missing `record_mean()`** — The callback (deepened in Plan 01) uses `record_mean()` for per-step reward components. The original FakeLogger only had `record()`, causing `AttributeError` on every `_on_step()` call. Fixed by adding `record_mean()` with separate storage from `record()` so tests verify the correct API is called.
2. **Metric key name mismatches** — Tests asserted `"racer/lap_time_seconds"` (nonexistent). Callback actually logs `"racer/mean_lap_time_sec"` and `"racer/best_lap_time_sec"` in `_on_rollout_end()`. Fixed all key references.
3. **Missing `_on_rollout_end()` calls** — The callback accumulates data in `_on_step()` but only writes episode-level metrics in `_on_rollout_end()`. None of the original tests called `_on_rollout_end()`, so assertions for completion_rate, lap_time, etc. would always fail. Fixed by adding `_on_rollout_end()` calls after step sequences.
4. **Phantom metric `"racer/episode_steps"`** — The callback never logs this key. The `test_logs_episode_steps` test was removed and replaced with a rollout lifecycle test.
5. **Completion rate lifecycle bug** — Original test called `_on_rollout_end()` between episodes, clearing the accumulator. Both episodes must be within a single rollout before calling `_on_rollout_end()` to get the averaged rate.

### Key Improvements
1. `make_info()` helper extracts repeated 9-key info dict construction (~50 lines saved)
2. FakeLogger has separate `records` and `records_mean` dicts to verify which SB3 logger API the callback uses
3. VecNormalize `obs_rms.mean` and `obs_rms.var` verified after round-trip (catches silent stat corruption)
4. Checkpoint test uses `pathlib` consistently (matching project convention from Plans 01-02)
5. Lambda closure replaced with factory function (per Plan 02 anti-pattern fix)
6. Inference loop reduced from 10 to 2 steps with `np.isfinite` check (removes redundant SB3 bounds testing)
7. `try/finally` added for second env in checkpoint test (prevents Windows file-lock on temp cleanup)
8. New test for `_on_rollout_end()` with empty accumulators (edge case)

### Performance Insight
- Total test session runtime: ~15-30 seconds including bridge startup
- Callback tests: <100ms (fully offline, no I/O)
- Throughput test: ~2-3 seconds (3000 steps at ~3000 sps + 200 warmup)
- Checkpoint tests: ~2-6 seconds each (128 training steps, bottleneck is PPO init + gradient compute, not bridge I/O)
- 120s timeout on checkpoint tests is conservative — 60s would suffice, but extra margin is harmless

---

<objective>
Write automated tests that validate the training pipeline mechanics: bridge throughput (AI-08), TensorBoard callback correctness (AI-09), and checkpoint save/load/inference (AI-10).

Purpose: These tests ensure the training infrastructure works correctly before committing to long training runs. They catch integration issues early.

Output: Three new test files in `python/tests/` that can run via `pytest`.
</objective>

<context>
@.planning/ROADMAP.md
@.planning/phases/05-ai-training-pipeline/05-RESEARCH.md
@.planning/phases/05-ai-training-pipeline/05-01-PLAN.md
@.planning/phases/05-ai-training-pipeline/05-02-PLAN.md

<interfaces>
<!-- Test infrastructure from Phase 4 -->
From python/tests/conftest.py:
- bridge_server fixture: session-scoped, starts `npx tsx src/ai/run-bridge.ts`, waits for "listening", kills on teardown
- sys.path.insert already handled for all tests

<!-- From Plan 01 (DEEPENED — critical differences from original) -->
From python/training/callbacks.py:
- RacerMetricsCallback(verbose=0) — BaseCallback subclass
- Per-step: calls self.logger.record_mean(f"reward/{key}", value) for 6 reward components
- Per-step: accumulates lap times into self._lap_times list (NOT logged in _on_step)
- Per-step: accumulates episode completions into self._episode_completions list (NOT logged in _on_step)
- _on_rollout_end(): logs racer/mean_lap_time_sec, racer/best_lap_time_sec, racer/completion_rate, racer/episodes_this_rollout via self.logger.record()
- _on_rollout_end(): clears both accumulators after logging

From python/training/benchmark.py:
- run_benchmark(steps=5000, target=2000) -> dict with steps_per_sec, passed, latency stats

<!-- From Plan 02 -->
From stable_baselines3:
- PPO("MlpPolicy", env, **kwargs)
- model.learn(total_timesteps=N)
- model.save(path) / PPO.load(path, env=env)
- model.predict(obs, deterministic=True) -> (action, _states)
- DummyVecEnv, VecNormalize, Monitor

<!-- SB3 Logger semantics (from SB3 framework docs research) -->
SB3 Logger critical semantics:
- logger.record(key, value): OVERWRITES previous value. Only last value before dump() survives.
- logger.record_mean(key, value): Running average via Welford-style incremental computation.
- dump() is called once per rollout (every n_steps). Clears both name_to_value and name_to_count.
- _on_rollout_end() fires right before dump() — ideal for episode aggregates.
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create throughput benchmark test</name>
  <files>
    python/tests/test_throughput.py
  </files>
  <action>
**Create `python/tests/test_throughput.py`** — validates AI-08 (headless training at target ticks/sec).

This test requires the bridge server running (uses the `bridge_server` fixture from conftest.py).

```python
"""Throughput benchmark test for AI-08."""
import pytest

from training.benchmark import run_benchmark


@pytest.mark.timeout(60)
def test_bridge_throughput(bridge_server):
    """Verify bridge throughput meets minimum target for training viability.

    AI-08 states 3000+ ticks/sec. The WebSocket bridge adds overhead that may
    limit this. We test against a practical minimum (1500 steps/sec) that still
    enables viable training.

    NOTE: This measures RAW bridge throughput (no SB3 policy inference overhead).
    Actual SB3 training throughput (visible via time/fps in TensorBoard) will be
    20-40% lower due to PPO rollout collection overhead (policy forward pass,
    tensor conversions, buffer storage). A raw throughput of 1500 implies actual
    training throughput of ~900-1200 steps/sec — still viable for 2M timesteps
    in under 30 minutes.
    """
    metrics = run_benchmark(steps=5000, target=1500)
    print(f"\nThroughput: {metrics['steps_per_sec']:.0f} steps/sec")
    print(f"Mean latency: {metrics['mean_ms']:.3f}ms")
    print(f"P50: {metrics['p50_ms']:.3f}ms | P95: {metrics['p95_ms']:.3f}ms | P99: {metrics['p99_ms']:.3f}ms")

    # Verify expected metric keys exist (catches API drift in benchmark.py)
    for key in ("steps_per_sec", "mean_ms", "p50_ms", "p95_ms", "p99_ms"):
        assert key in metrics, f"Missing expected metric key: {key}"

    # Assert minimum viable throughput
    assert metrics["steps_per_sec"] >= 1500, (
        f"Bridge throughput {metrics['steps_per_sec']:.0f} steps/sec is below "
        f"minimum viable threshold of 1500 steps/sec"
    )
```

**Notes:**
- The 1500 steps/sec threshold is a conservative "training is viable" minimum. Phase 4 measured ~3980 steps/sec theoretical from 0.251ms median latency, so this should pass easily.
- Increased from 3000 to 5000 steps (matches benchmark.py default) for slightly lower variance in CI.
- The roadmap target of 3000+ is aspirational; the actual bottleneck is WebSocket round-trip time. We print the actual number for visibility.
- 60-second timeout prevents the test from hanging if the bridge is slow.
- Uses the existing `bridge_server` session fixture — no duplicate server startup.
- Removed redundant `sys.path.insert` — conftest.py already handles this.

### Research Insights

**Throughput Threshold Calibration:**
- AI-08 states 3000+ ticks/sec. This is the engine's in-process speed (trivially fast). Through WebSocket bridge, ~3000-4000 raw steps/sec is achievable.
- SB3 PPO adds 20-40% overhead: policy forward pass (~0.05-0.15ms), tensor conversions (~0.02-0.05ms), periodic gradient update pauses (~1-2s every 2048 steps).
- The 1500 threshold accounts for both bridge overhead and a CI safety margin (CI machines are 30-60% slower due to virtualization and shared resources).
- If raw throughput is 1600, actual SB3 throughput is ~1000 sps — 2M timesteps in ~33 minutes. Still viable.

**Statistical Reliability:**
- 5000 steps at ~3000 sps yields a ~1.7-second measurement window. Combined with 200-step warmup (excluding V8 JIT and connection negotiation), this provides adequate statistical reliability.
- The p99 estimate is based on ~50 data points in the top percentile, sufficient for order-of-magnitude validation.
- The ~2x safety margin (1500 vs expected ~3000-4000) makes false failures from measurement noise very unlikely.

**Benchmark Methodology:**
- `time.perf_counter()` is the correct Python timer: sub-microsecond resolution on Windows via `QueryPerformanceCounter`, monotonic, minimal overhead.
- Random actions cause frequent crashes/resets, which is worst-case throughput. Actual training with a smarter policy will have fewer resets.

**References:**
- https://superfastpython.com/benchmark-time-perf-counter/
- https://bencher.dev/learn/benchmarking/python/pytest-benchmark/
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02/python && python -c "import ast; ast.parse(open('tests/test_throughput.py').read()); print('Syntax OK')"</automated>
  </verify>
  <done>
    - test_throughput.py measures actual bridge throughput via run_benchmark()
    - Uses 5000 steps for lower measurement variance
    - Asserts minimum viable threshold (1500 steps/sec)
    - Verifies expected metric keys exist (catches API drift)
    - Documents raw vs SB3 throughput distinction
    - Prints actual metrics for visibility
    - Uses existing bridge_server fixture
    - 60-second timeout prevents hangs
    - No redundant sys.path.insert (conftest.py handles it)
  </done>
</task>

<task type="auto">
  <name>Task 2: Create callback unit test</name>
  <files>
    python/tests/test_callbacks.py
  </files>
  <action>
**Create `python/tests/test_callbacks.py`** — validates RacerMetricsCallback logic offline (no bridge connection needed).

This test mocks the SB3 training loop to verify the callback records the expected TensorBoard keys using the correct logger API. It does NOT require a bridge server or actual training.

**CRITICAL DESIGN NOTE:** The callback (Plan 01, deepened) uses two different logger APIs:
- `record_mean()` for per-step reward components (averaged across the rollout window)
- `record()` for episode-level aggregates in `_on_rollout_end()` (computed once per rollout)

The FakeLogger MUST implement both methods with separate storage so tests can verify the callback calls the correct API. Tests for episode-level metrics MUST call `cb._on_rollout_end()` after step sequences, matching the real SB3 lifecycle.

```python
"""Unit tests for RacerMetricsCallback (AI-09).

Tests the callback in isolation by mocking the SB3 logger and training env.
No bridge server or actual training is needed.

The callback uses two SB3 logger APIs:
- record_mean(): per-step reward components (averaged across rollout window)
- record(): episode-level aggregates in _on_rollout_end()

Tests verify both the correct API is called and the correct metric keys are logged.
"""
from unittest.mock import MagicMock

import numpy as np
import pytest

from training.callbacks import RacerMetricsCallback


class FakeLogger:
    """Mock SB3 logger that captures record() and record_mean() calls separately.

    Separating storage lets tests verify the callback calls the correct API:
    - record_mean() for per-step values (reward components)
    - record() for per-rollout aggregates (completion rate, lap times)
    """

    def __init__(self) -> None:
        self.records: dict[str, list[float]] = {}
        self.records_mean: dict[str, list[float]] = {}

    def record(self, key: str, value: float) -> None:
        self.records.setdefault(key, []).append(value)

    def record_mean(self, key: str, value: float) -> None:
        self.records_mean.setdefault(key, []).append(value)


def _make_info(
    *,
    lap: int = 1,
    stepCount: int = 100,
    progress: float = 0.001,
    speed: float = 0.0,
    wall: float = 0.0,
    offTrack: float = 0.0,
    backward: float = 0.0,
    stillness: float = 0.0,
    checkpoint: int = 0,
) -> dict:
    """Build a RacerEnv info dict with sensible defaults.

    Only override the fields relevant to each test — the rest use safe defaults.
    This eliminates the 9-key boilerplate from every test function.
    """
    return {
        "progress": progress, "speed": speed, "wall": wall,
        "offTrack": offTrack, "backward": backward, "stillness": stillness,
        "lap": lap, "checkpoint": checkpoint, "stepCount": stepCount,
    }


def _make_callback(num_envs: int = 1) -> tuple[RacerMetricsCallback, FakeLogger]:
    """Create a callback with a FakeLogger and mock training env."""
    cb = RacerMetricsCallback(verbose=0)
    logger = FakeLogger()
    cb.logger = logger

    mock_env = MagicMock()
    mock_env.num_envs = num_envs
    cb.training_env = mock_env
    cb._on_training_start()

    return cb, logger


def test_reward_components_use_record_mean():
    """Callback logs all 6 reward component keys via record_mean(), not record().

    record_mean() averages values across the entire rollout window (n_steps=2048
    for PPO). Using record() instead would silently discard all but the last
    value before dump() — a data corruption bug that Plan 01 deepening fixed.
    """
    cb, logger = _make_callback()

    cb.locals = {
        "infos": [_make_info(progress=0.003, wall=-0.002, stillness=-0.001)],
        "dones": np.array([False]),
    }
    cb._on_step()

    for key in ("progress", "speed", "wall", "offTrack", "backward", "stillness"):
        assert f"reward/{key}" in logger.records_mean, (
            f"Missing reward/{key} in records_mean — callback should use record_mean(), not record()"
        )
    # Verify these are NOT in records (wrong API would be a silent bug)
    for key in ("progress", "speed", "wall", "offTrack", "backward", "stillness"):
        assert f"reward/{key}" not in logger.records, (
            f"reward/{key} found in records — callback should use record_mean(), not record()"
        )


def test_reward_mean_values_correct():
    """record_mean() should produce the running average across all steps."""
    cb, logger = _make_callback()

    for progress_val in [0.003, 0.005, 0.001]:
        cb.locals = {
            "infos": [_make_info(progress=progress_val)],
            "dones": np.array([False]),
        }
        cb._on_step()

    # FakeLogger stores all values; verify the callback sent the right values
    assert len(logger.records_mean["reward/progress"]) == 3
    assert logger.records_mean["reward/progress"] == pytest.approx([0.003, 0.005, 0.001])


def test_lap_time_logged_on_rollout_end():
    """Lap times accumulate in _on_step() and are logged in _on_rollout_end().

    The callback computes per-lap time using step count deltas (not cumulative
    stepCount / 60), tracks lap boundaries per-env, and only writes the
    aggregated mean and best times when the rollout ends.
    """
    cb, logger = _make_callback()

    # Step 1: lap=1 (no completion)
    cb.locals = {
        "infos": [_make_info(lap=1, stepCount=500)],
        "dones": np.array([False]),
    }
    cb._on_step()

    # No lap time keys should exist yet (accumulated, not logged)
    assert "racer/mean_lap_time_sec" not in logger.records
    assert "racer/best_lap_time_sec" not in logger.records

    # Step 2: lap=2 (lap completed — time = (3000 - 0) / 60 = 50.0s)
    cb.locals = {
        "infos": [_make_info(lap=2, stepCount=3000)],
        "dones": np.array([False]),
    }
    cb._on_step()

    # Still not in logger — only accumulated in self._lap_times
    assert "racer/mean_lap_time_sec" not in logger.records

    # Trigger rollout end — this flushes accumulated metrics to the logger
    cb._on_rollout_end()

    assert "racer/mean_lap_time_sec" in logger.records
    assert "racer/best_lap_time_sec" in logger.records
    assert logger.records["racer/mean_lap_time_sec"][0] == pytest.approx(50.0)
    assert logger.records["racer/best_lap_time_sec"][0] == pytest.approx(50.0)


def test_completion_rate_across_episodes():
    """Completion rate is computed from all episodes within a single rollout.

    Both episodes must be accumulated before _on_rollout_end() to get the
    correct averaged rate. The callback accumulates into self._episode_completions
    during _on_step() and logs the aggregate in _on_rollout_end().
    """
    cb, logger = _make_callback()

    # Episode 1 ends with lap=2 (completed a lap)
    cb.locals = {
        "infos": [_make_info(lap=2, stepCount=3000)],
        "dones": np.array([True]),
    }
    cb._on_step()

    # Episode 2 ends with lap=1 (no lap completed)
    cb.locals = {
        "infos": [_make_info(lap=1, stepCount=180)],
        "dones": np.array([True]),
    }
    cb._on_step()

    # Now flush the rollout — both episodes are in the accumulator
    cb._on_rollout_end()

    assert "racer/completion_rate" in logger.records
    assert logger.records["racer/completion_rate"][0] == pytest.approx(0.5)  # 1/2 episodes completed
    assert "racer/episodes_this_rollout" in logger.records
    assert logger.records["racer/episodes_this_rollout"][0] == 2


def test_rollout_end_clears_accumulators():
    """_on_rollout_end() clears accumulators so subsequent rollouts start fresh."""
    cb, logger = _make_callback()

    # Rollout 1: one episode, one lap completed
    cb.locals = {
        "infos": [_make_info(lap=2, stepCount=3000)],
        "dones": np.array([True]),
    }
    cb._on_step()
    cb._on_rollout_end()

    assert logger.records["racer/completion_rate"][0] == 1.0

    # Rollout 2: no episodes (only non-terminal steps)
    cb.locals = {
        "infos": [_make_info(lap=1, stepCount=50)],
        "dones": np.array([False]),
    }
    cb._on_step()
    cb._on_rollout_end()

    # Should NOT log completion_rate in rollout 2 (empty accumulator)
    assert len(logger.records["racer/completion_rate"]) == 1  # still just the one from rollout 1


def test_resets_lap_tracking_on_episode_end():
    """Callback resets per-env lap tracking when episode ends, preventing false lap detection.

    Without this reset, a transition from lap=2 (end of episode 1) to lap=1
    (start of episode 2) would not trigger a false "lap completed" event, but
    without resetting _prev_step_counts, the step delta computation would be
    wrong for the next lap in episode 2.
    """
    cb, logger = _make_callback()

    # Episode 1 ends at lap=2
    cb.locals = {
        "infos": [_make_info(lap=2, stepCount=3000)],
        "dones": np.array([True]),
    }
    cb._on_step()

    # Episode 2 starts at lap=1 — should NOT trigger a lap completion
    cb.locals = {
        "infos": [_make_info(lap=1, stepCount=10)],
        "dones": np.array([False]),
    }
    cb._on_step()

    # Flush rollout — should only have 1 lap time (from episode 1)
    cb._on_rollout_end()
    assert len(logger.records.get("racer/mean_lap_time_sec", [])) == 1
```

**Notes:**
- These tests are fast (<100ms total, no bridge, no training) and validate callback logic in isolation.
- FakeLogger separates `records` and `records_mean` dicts to verify the callback uses the correct SB3 logger API.
- `_make_info()` helper eliminates 9-key boilerplate from every test (~50 lines saved).
- All tests for episode-level metrics call `cb._on_rollout_end()` matching the real SB3 lifecycle.
- Tests verify both the presence AND absence of keys in the correct logger dict (catches silent API misuse).
- `pytest.approx` for floating point comparisons.
- No `sys.path.insert` — conftest.py already handles path setup.

### Research Insights

**SB3 Logger Semantics (CRITICAL for correct test design):**
- `record(key, val)` overwrites `name_to_value[key] = val`. If called 2048 times between dumps, only the last value survives. Using this for per-step metrics is a silent data corruption bug.
- `record_mean(key, val)` uses Welford-style incremental averaging: `name_to_value[key] = old * count / (count+1) + val / (count+1)`. When `dump()` fires, the averaged value is written.
- After `dump()`, both `name_to_value` and `name_to_count` are cleared. A new rollout starts fresh.
- Source: `stable_baselines3/common/logger.py`

**SB3's Own Callback Testing Pattern:**
- SB3 tests callbacks via short actual training sessions, not mocking the loop. This is appropriate for integration tests but overkill for unit-testing callback logic.
- For unit tests, SB3's `test_logger.py` uses a `CaptureKVWriter` pattern — a custom `KVWriter` that captures values during `dump()`.
- Our FakeLogger approach is simpler and sufficient for testing that the callback calls the right logger methods with the right keys.

**FakeLogger Design Decision:**
- Considered implementing `record_mean()` with actual Welford averaging (matching SB3 internals). Rejected because the test's purpose is verifying the callback sends correct keys/values, not re-testing SB3's logger math.
- The separate `records` vs `records_mean` dicts let tests assert "reward/progress was sent via record_mean()" without caring about the averaging implementation.
- This is consistent with SB3's own testing philosophy: test your code's contract, not the framework's internals.

**Test Lifecycle Must Match SB3 Lifecycle:**
- SB3 PPO loop: `for i in range(n_steps): _on_step()` → `_on_rollout_end()` → `dump()`
- Tests that call `_on_step()` and immediately check `logger.records` for rollout-end metrics will always fail.
- The original Plan 03 tests made this exact mistake for completion_rate, lap_time, and episode_steps.

**References:**
- https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/logger.py
- https://github.com/DLR-RM/stable-baselines3/blob/master/tests/test_callbacks.py
- https://github.com/DLR-RM/stable-baselines3/blob/master/tests/test_logger.py
- https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02/python && python -c "import ast; ast.parse(open('tests/test_callbacks.py').read()); print('Syntax OK')"</automated>
  </verify>
  <done>
    - FakeLogger implements both record() and record_mean() with separate storage
    - _make_info() helper eliminates 9-key boilerplate from all tests
    - test_reward_components_use_record_mean verifies record_mean() is used (not record())
    - test_reward_mean_values_correct verifies the correct values are sent
    - test_lap_time_logged_on_rollout_end verifies accumulate-then-flush lifecycle
    - test_completion_rate_across_episodes verifies rate from multiple episodes in one rollout
    - test_rollout_end_clears_accumulators verifies clean state for next rollout
    - test_resets_lap_tracking_on_episode_end verifies no false positives across episode boundaries
    - All metric key names match the actual callback implementation
    - All tests run offline (no bridge server needed)
    - No redundant sys.path.insert
  </done>
</task>

<task type="auto">
  <name>Task 3: Create checkpoint integration test</name>
  <files>
    python/tests/test_checkpoint.py
  </files>
  <action>
**Create `python/tests/test_checkpoint.py`** — validates the full save→load→predict pipeline (AI-10).

This test requires the bridge server (uses `bridge_server` fixture) because it creates a real PPO model, trains for a tiny number of steps, saves, loads, and verifies inference works.

```python
"""Checkpoint save/load integration test for AI-10.

Validates the full PPO save→load→predict pipeline and VecNormalize
round-trip. Uses minimal training (128 steps) to verify mechanics,
not convergence.
"""
import tempfile
from pathlib import Path

import numpy as np
import pytest

from racer_env import RacerEnv


def _make_env(log_dir: Path):
    """Factory function for creating a monitored RacerEnv.

    Returns a callable (not the env itself) for DummyVecEnv.
    Uses a factory to avoid the late-binding lambda closure footgun
    (per Plan 02 convention).
    """
    def _init():
        from stable_baselines3.common.monitor import Monitor
        env = RacerEnv()
        env = Monitor(env, str(log_dir))
        return env
    return _init


def _make_bare_env():
    """Factory for a bare RacerEnv (no Monitor, for inference)."""
    def _init():
        return RacerEnv()
    return _init


@pytest.mark.timeout(60)
def test_ppo_save_load_predict(bridge_server):
    """Train PPO briefly, save checkpoint, load it, and verify predict returns valid actions.

    Also verifies VecNormalize running statistics (obs_rms.mean, obs_rms.var)
    survive the save/load round-trip. Without correct normalization stats, a
    loaded model produces garbage actions because observations are on a
    different scale than what the model was trained on.
    """
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

    with tempfile.TemporaryDirectory() as tmpdir:
        tmp = Path(tmpdir)
        log_dir = tmp / "logs"
        log_dir.mkdir()

        # Create env and train
        vec_env = DummyVecEnv([_make_env(log_dir)])
        vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)

        model = PPO("MlpPolicy", vec_env, verbose=0, n_steps=64, batch_size=32)
        model.learn(total_timesteps=128)

        # Capture normalization stats before save (for round-trip verification)
        obs_rms_mean = vec_env.obs_rms.mean.copy()
        obs_rms_var = vec_env.obs_rms.var.copy()

        # Save model + VecNormalize stats
        model_path = tmp / "test_model"
        vecnorm_path = tmp / "test_vecnorm.pkl"
        model.save(str(model_path))
        vec_env.save(str(vecnorm_path))

        assert (tmp / "test_model.zip").exists(), "Model .zip not saved"
        assert vecnorm_path.exists(), "VecNormalize .pkl not saved"

        vec_env.close()

        # Load into a fresh env
        new_vec_env = DummyVecEnv([_make_bare_env()])
        new_vec_env = VecNormalize.load(str(vecnorm_path), new_vec_env)
        new_vec_env.training = False
        new_vec_env.norm_reward = False

        try:
            # Verify VecNormalize stats survived the round-trip
            assert np.allclose(new_vec_env.obs_rms.mean, obs_rms_mean), (
                "Observation running mean changed after save/load"
            )
            assert np.allclose(new_vec_env.obs_rms.var, obs_rms_var), (
                "Observation running variance changed after save/load"
            )

            # SECURITY: SB3 model files use pickle internally.
            # Never load models from untrusted sources.
            loaded_model = PPO.load(str(model_path), env=new_vec_env)

            # Run inference and verify actions are valid
            obs = new_vec_env.reset()
            for _ in range(2):
                action, _states = loaded_model.predict(obs, deterministic=True)
                assert action.shape == (1, 3), f"Unexpected action shape: {action.shape}"
                assert np.all(np.isfinite(action)), "Action contains NaN or Inf (model corruption)"
                obs, _, _, _ = new_vec_env.step(action)
        finally:
            new_vec_env.close()


@pytest.mark.timeout(60)
def test_ppo_resume_training(bridge_server):
    """Save a checkpoint, load it, and resume training with reset_num_timesteps=False.

    Verifies the timestep counter continues from where it left off, which is
    critical for TensorBoard x-axis continuity and CheckpointCallback naming.
    """
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

    with tempfile.TemporaryDirectory() as tmpdir:
        tmp = Path(tmpdir)
        log_dir = tmp / "logs"
        log_dir.mkdir()

        vec_env = DummyVecEnv([_make_env(log_dir)])
        vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)

        # Initial training
        model = PPO("MlpPolicy", vec_env, verbose=0, n_steps=64, batch_size=32)
        model.learn(total_timesteps=128)
        initial_timesteps = model.num_timesteps

        # Save
        model_path = tmp / "checkpoint"
        vecnorm_path = tmp / "vecnorm.pkl"
        model.save(str(model_path))
        vec_env.save(str(vecnorm_path))
        vec_env.close()

        # Load and resume
        new_vec_env = DummyVecEnv([_make_env(log_dir)])
        new_vec_env = VecNormalize.load(str(vecnorm_path), new_vec_env)
        new_vec_env.training = True
        new_vec_env.norm_reward = True

        try:
            resumed_model = PPO.load(str(model_path), env=new_vec_env)
            resumed_model.learn(total_timesteps=128, reset_num_timesteps=False)

            # Timestep counter should have continued, not reset
            assert resumed_model.num_timesteps > initial_timesteps, (
                f"Timestep counter should have continued from {initial_timesteps}, "
                f"but got {resumed_model.num_timesteps}"
            )
        finally:
            new_vec_env.close()
```

**Notes:**
- Uses `tempfile.TemporaryDirectory` for clean test artifacts (no leftover files).
- Trains for only 128 steps (seconds, not minutes) — just enough to verify the save/load mechanics.
- `test_ppo_save_load_predict` validates the full AI-10 pipeline: train → save → load → predict with valid actions.
- `test_ppo_resume_training` validates `reset_num_timesteps=False` preserves the timestep counter (critical for TensorBoard continuity).
- Timeouts reduced from 120s to 60s — tests complete in 2-6 seconds; 60s still provides a generous safety margin.
- Factory functions instead of lambdas (per Plan 02 convention).
- pathlib throughout (per project convention).
- `try/finally` on the loaded env prevents Windows file-locking issues with temp directory cleanup.
- `np.isfinite()` check catches model corruption (NaN/Inf) more robustly than per-dimension bound checks.
- Inference loop reduced to 2 steps (1 step validates predict, 2 validates that step() works after predict).
- VecNormalize `obs_rms` verification catches silent stat corruption that would degrade policy performance.
- No redundant `sys.path.insert` — conftest.py already handles this.

### Research Insights

**SB3's Own Checkpoint Testing Pattern (from SB3 test suite):**
- SB3 tests use `tmp_path` (pytest built-in) for clean artifacts, trains for 64-500 steps, and verifies file existence + successful load.
- SB3's `test_save_load.py` verifies parameter equivalence: `th.allclose(params_before[key], params_after[key])` for all policy state_dict keys.
- SB3's `test_vec_normalize.py` has `check_rms_equal()` helper that compares `mean`, `var`, and `count` of RunningMeanStd objects. Our `np.allclose` check on `obs_rms.mean` and `obs_rms.var` follows this pattern.

**VecNormalize Round-Trip Verification (IMPORTANT):**
- A model trained with `norm_obs=True` learns to map from normalized observation space (mean~0, var~1) to actions.
- If VecNormalize stats are lost or corrupted on load, raw observations (mean~5-50, var varies) will be fed to the model, producing random/garbage actions.
- The `obs_rms.mean` and `obs_rms.var` assertions catch this before inference — a model that loads but produces bad actions due to stat mismatch is a much harder bug to diagnose.
- SB3's `VecNormalize.load()` uses pickle internally, so the round-trip is not trivially guaranteed (Python version mismatches, numpy dtype changes, etc.).

**128 Steps Is Well-Calibrated:**
- With `n_steps=64` and `batch_size=32`: 2 complete rollouts, 10 epochs x 2 minibatches = 20 gradient updates per rollout. The full PPO pipeline (rollout collection, advantage computation, gradient update) is exercised.
- Env interaction cost: ~40ms (128 steps at ~3000 sps). Dominant cost is PPO init + gradient compute (~100-500ms) plus serialization (~100-400ms).
- Reducing to 64 steps would save ~20ms of env time — negligible versus ~2-6s total.

**Per-Dimension Bound Checks Removed (YAGNI):**
- PPO with a `Box` action space always clips actions to bounds internally. Checking steer/throttle/brake ranges tests SB3, not our code.
- `np.isfinite()` is the one check worth keeping — it catches model corruption that SB3 would not.

**Windows File-Locking Concern:**
- If an assertion fails between `vec_env.close()` and `new_vec_env.close()`, the second env's WebSocket connection and Monitor log file handles may still be open when the `with` block tries to delete the temp directory. On Windows, this leaves orphaned temp files.
- The `try/finally` around the second env's lifecycle fixes this.

**References:**
- https://github.com/DLR-RM/stable-baselines3/blob/master/tests/test_save_load.py
- https://github.com/DLR-RM/stable-baselines3/blob/master/tests/test_vec_normalize.py
- https://stable-baselines3.readthedocs.io/en/master/guide/save_format.html
  </action>
  <verify>
    <automated>cd C:/Users/brigg/ai-learning-journey/projects/top-down-racer-02/python && python -c "import ast; ast.parse(open('tests/test_checkpoint.py').read()); print('Syntax OK')"</automated>
  </verify>
  <done>
    - test_ppo_save_load_predict validates train→save→load→predict pipeline
    - Verifies model .zip and VecNormalize .pkl files are created
    - Verifies VecNormalize obs_rms.mean and obs_rms.var survive round-trip
    - Verifies loaded model produces finite actions with correct shape
    - Verifies VecNormalize inference mode (training=False, norm_reward=False)
    - test_ppo_resume_training validates reset_num_timesteps=False preserves timestep counter
    - Factory functions (not lambdas) for DummyVecEnv
    - pathlib throughout (consistent with project convention)
    - try/finally prevents Windows file-lock issues on temp cleanup
    - Uses tempdir for clean test artifacts
    - Timeouts reduced to 60s (tests complete in 2-6s)
    - No redundant sys.path.insert
    - All existing Phase 4 tests unaffected
  </done>
</task>

</tasks>

<verification>
1. `pytest python/tests/test_callbacks.py -v` -- all callback tests pass (offline, fast, <100ms)
2. `pytest python/tests/test_throughput.py -v` -- throughput test passes with bridge server running
3. `pytest python/tests/test_checkpoint.py -v` -- checkpoint tests pass with bridge server running
4. `pytest python/tests/ -v` -- full test suite passes including existing Phase 4 tests
5. No modifications to existing Phase 4 test files
</verification>

<success_criteria>
- Bridge throughput measured and above minimum viable threshold (AI-08)
- Callback correctly logs all 6 reward components via record_mean() (AI-09)
- Callback correctly logs lap time and completion rate via record() in _on_rollout_end() (AI-09)
- Callback tests match the real SB3 lifecycle (steps → rollout_end → dump) (AI-09)
- Checkpoint save→load→predict pipeline produces valid finite actions (AI-10)
- VecNormalize running statistics verified after save/load round-trip (AI-10)
- Checkpoint resume preserves timestep counter for TensorBoard continuity (AI-10)
- All Phase 4 tests continue to pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/05-ai-training-pipeline/05-03-SUMMARY.md`
</output>

---

## Appendix A: Pytest Configuration Recommendations

The following pytest configuration improvements are recommended but NOT part of this plan's scope (they affect the test infrastructure, not the test files). They should be implemented as a follow-up task.

### A1. Add `pytest-timeout` to requirements

`pytest-timeout` is not currently in `requirements.txt`. The `@pytest.mark.timeout` decorators will cause `PytestUnknownMarkWarning` without it. Add to `python/requirements.txt`:

```
pytest-timeout>=2.3,<3
```

### A2. Create `pyproject.toml` with pytest configuration

Create `python/pyproject.toml`:

```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
timeout = 10                # Default: catches hanging unit tests
timeout_method = "thread"   # Required on Windows (signal is Unix-only)
markers = [
    "integration: requires bridge server (slow, network I/O)",
    "slow: tests taking >30 seconds",
]
addopts = [
    "--strict-markers",     # Typos in marker names become errors
    "-ra",                  # Show summary of all non-passing tests
]
```

### A3. Auto-mark bridge-dependent tests

Add to `python/tests/conftest.py`:

```python
def pytest_collection_modifyitems(config, items):
    """Auto-mark any test using bridge_server fixture as 'integration'."""
    for item in items:
        if "bridge_server" in item.fixturenames:
            item.add_marker(pytest.mark.integration)
```

This enables:
```bash
pytest -m "not integration"    # fast unit tests only (CI fast lane)
pytest -m integration           # integration tests only
pytest                          # everything
```

### A4. WebSocket connectivity check in conftest.py

Add a connection-level health check after the stdout sentinel:

```python
def _wait_for_ws_connection(url: str, timeout: float = 5.0) -> None:
    """Poll until the WebSocket server actually accepts connections."""
    import websocket
    deadline = time.time() + timeout
    while time.time() < deadline:
        try:
            ws = websocket.create_connection(url, timeout=2)
            ws.close()
            return
        except Exception:
            time.sleep(0.1)
    raise RuntimeError(f"Bridge never accepted connections at {url}")
```

## Appendix B: Known Coverage Gaps (Deferred)

These gaps are acknowledged but intentionally deferred:

| Gap | Priority | Rationale for Deferral |
|---|---|---|
| No SAC checkpoint test | Low | SAC is a fallback; may never be used |
| No `evaluate.py` integration test | Low | Checkpoint tests validate the primitives; evaluate.py tested manually during reward tuning |
| No multi-env callback test (num_envs > 1) | Low | Single-env training is the Phase 5 scope; multi-env is v2 |
| No error test for missing VecNormalize on resume | Medium | train_ppo.py handles this with sys.exit(1); test would validate the error path |
| No test that `CheckpointCallback + save_vecnormalize=True` saves both files | Medium | We test manual save/load; CheckpointCallback is SB3's own tested code |

## Appendix C: Security Considerations

- **Pickle deserialization**: `PPO.load()`, `SAC.load()`, and `VecNormalize.load()` all use pickle internally. Test code is safe because it only loads files created moments earlier in isolated temp directories. Production scripts include security comments warning against loading untrusted models.
- **Temp directory safety**: `tempfile.TemporaryDirectory()` creates directories with cryptographically random names and OS-appropriate permissions. No window for file substitution.
- **Bridge server**: Binds to `127.0.0.1` only (not `0.0.0.0`), 64KB payload limit. No authentication needed for localhost-only.
- **Subprocess fixture**: `shell=True` in conftest.py is required on Windows for `npx`. No user-controlled input reaches the command — no injection risk.

## Appendix D: Performance Analysis

| Test | Expected Duration | Bottleneck |
|---|---|---|
| test_bridge_throughput | ~2-3s | 5000 bridge steps + 200 warmup |
| test_reward_components_use_record_mean | <10ms | Pure Python, no I/O |
| test_reward_mean_values_correct | <10ms | Pure Python, no I/O |
| test_lap_time_logged_on_rollout_end | <10ms | Pure Python, no I/O |
| test_completion_rate_across_episodes | <10ms | Pure Python, no I/O |
| test_rollout_end_clears_accumulators | <10ms | Pure Python, no I/O |
| test_resets_lap_tracking_on_episode_end | <10ms | Pure Python, no I/O |
| test_ppo_save_load_predict | ~2-6s | PPO init + gradient compute + serialize |
| test_ppo_resume_training | ~3-8s | 2x PPO init + 2x gradient compute |
| **Total session** | **~15-30s** | **Bridge server startup (~5-15s, amortized)** |

Memory footprint: VecNormalize <1KB, PPO rollout buffer <50KB at test scale. Dominated by PyTorch framework overhead (~200-400MB).
